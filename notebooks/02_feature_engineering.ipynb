{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Temporal Features\n",
    "\n",
    "This notebook extracts and analyzes temporal features from engagement time series data.\n",
    "\n",
    "Features include:\n",
    "- Rolling statistics (mean, std, min, max)\n",
    "- Ratio features (likes/views, comments/views)\n",
    "- Burst detection (peaks, max/mean ratio)\n",
    "- Autocorrelation at different lags\n",
    "- Entropy and regularity measures\n",
    "- Trend features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# add project root to path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "\n",
    "# create output directory for plots\n",
    "\n",
    "output_dir = project_root / \"outputs\" / \"figures\"\n",
    "\n",
    "output_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# set plotting style\n",
    "\n",
    "try:\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "except OSError:\n",
    "\n",
    "try:\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "except OSError:\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "\n",
    "\n",
    "# try to import PCA / UMAP for visualization\n",
    "\n",
    "try:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "HAS_PCA = True\n",
    "\n",
    "except ImportError:\n",
    "\n",
    "HAS_PCA = False\n",
    "\n",
    "print(\"Warning: sklearn not available for PCA visualization\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "import umap\n",
    "\n",
    "HAS_UMAP = True\n",
    "\n",
    "except ImportError:\n",
    "\n",
    "HAS_UMAP = False\n",
    "\n",
    "print(\"Warning: umap - learn not available for UMAP visualization\")\n",
    "\n",
    "\n",
    "\n",
    "# import project modules\n",
    "\n",
    "from src.data.load_data import load_data\n",
    "\n",
    "from src.features.temporal_features import(\n",
    "\n",
    "extract_temporal_features,\n",
    "\n",
    "save_features,\n",
    "\n",
    "compute_rolling_statistics,\n",
    "\n",
    "compute_ratios,\n",
    "\n",
    "detect_bursts,\n",
    "\n",
    "compute_autocorrelation,\n",
    "\n",
    "compute_entropy,\n",
    "\n",
    "compute_trend_features,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# import IPython display for showing saved images\n",
    "\n",
    "try:\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "HAS_IPYTHON = True\n",
    "\n",
    "except ImportError:\n",
    "\n",
    "HAS_IPYTHON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load the preprocessed time series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "data_path = project_root / \"data\" / \"raw\" / \"engagement.parquet\"\n",
    "\n",
    "df = load_data(data_path)\n",
    "\n",
    "\n",
    "\n",
    "# adapt column names if needed\n",
    "\n",
    "if 'user_id' in df.columns and 'id' not in df.columns:\n",
    "\n",
    "df['id'] = df['user_id']\n",
    "\n",
    "if 'is_fake_series' in df.columns and 'label' not in df.columns:\n",
    "\n",
    "df['label'] = df['is_fake_series'].map({True: 'fake', False: 'normal'})\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "print(f\"Number of unique users: {df['id'].nunique()}\")\n",
    "\n",
    "\n",
    "\n",
    "if 'is_fake_series' in df.columns:\n",
    "\n",
    "print(f\"\\nFake series distribution:\")\n",
    "\n",
    "print(df['is_fake_series'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "if 'label' in df.columns:\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Temporal Features\n",
    "\n",
    "Extract all temporal features for each video ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features aggregated per user ID\n",
    "\n",
    "print(\"Extracting temporal features...\")\n",
    "\n",
    "features_df = extract_temporal_features(\n",
    "\n",
    "df,\n",
    "\n",
    "id_column = \"id\",\n",
    "\n",
    "timestamp_column = \"timestamp\",\n",
    "\n",
    "window_sizes = [6, 12, 24],\n",
    "\n",
    "autocorr_lags = [1, 6, 12, 24],\n",
    "\n",
    "aggregate_per_id = True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nFeatures extracted: {features_df.shape}\")\n",
    "\n",
    "print(f\"Number of feature columns: {len([c for c in features_df.columns if c not in ['id', 'label']])}\")\n",
    "\n",
    "\n",
    "\n",
    "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
    "\n",
    "print(f\"\\nFeature columns(first 20):\")\n",
    "\n",
    "print(feature_cols[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rolling Features Visualization\n",
    "\n",
    "Show rolling mean, variance, and autocorrelation features for sample series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a sample user\n",
    "\n",
    "seriessample_user_id = df['id'].unique()[0]\n",
    "\n",
    "sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "\n",
    "# compute rolling features\n",
    "\n",
    "for this seriesrolling_mean_6 = sample_series['views'].rolling(window = 6, min_periods = 1).mean()rolling_std_6 = sample_series['views'].rolling(window = 6, min_periods = 1).std()rolling_mean_24 = sample_series['views'].rolling(window = 24, min_periods = 1).mean()rolling_std_24 = sample_series['views'].rolling(window = 24, min_periods = 1).std()",
    "\n",
    "# compute autocorrelation(simplified - using rolling correlation)autocorr_lag_1 = sample_series['views'].rolling(window = 12, min_periods = 2).apply(lambda x: x.corr(x.shif\n",
    "t(1))\n",
    "\n",
    "if len(x.dropna()) > 1 else 0, raw = False)\n",
    "\n",
    "# plot series with rolling feature\n",
    "s(2 - row panel)fig,\n",
    "\n",
    "axes = plt.subplots(2, 1, figsize = (16, 10), sharex = True)\n",
    "\n",
    "# top panel: original seriesax1 = axes[0]ax1.plot(sample_series['timestamp'], sample_series['views'], label = 'Views', linewidth = 2, color = 'blue', alpha = 0.7)ax1.plot(sample_series['timestamp'], rolling_mean_6, label = 'Rolling Mean(6h)', linewidth = 1.5, color = 'green', linestyle = ' -- ')ax1.plot(sample_series['timestamp'], rolling_mean_24, label = 'Rolling Mean(24h)', linewidth = 1.5, color = 'orange', linestyle = ' -- ')ax1.fill_between(sample_series['timestamp'], rolling_mean_6 - rolling_std_6, rolling_mean_6 + rolling_std_6, alpha = 0.2, color = 'green', label = 'Rolling Std(6h)')ax1.set_ylabel('Views', fontsize = 12)ax1.set_title(f'Time Series with Rolling Statistics - User: {sample_user_id}', fontsize = 14, fontweight = 'bold')ax1.legend()ax1.gr",
    "i",
    "d(True, alpha = 0.3)\n",
    "\n",
    "# bottom panel: feature scoresax2 = axes[1]ax2.plot(sample_series['timestamp'], rolling_std_6, label = 'Rolling Std(6h)', linewidth = 1.5, color = 'red')ax2.plot(sample_series['timestamp'], rolling_std_24, label = 'Rolling S",
    "t",
    "d(24h)', linewidth = 1.5, color = 'purple')ax2\n",
    "\n",
    "_twin = ax2.twinx()ax2_twin.plot(sample_series['timestamp'], autocorr_lag_1, label = 'Autocorr(lag = 1)', linewidth = 1.5, color = 'brown', linestyle = ':')ax2_twin.set_ylabel('Autocorrelation', fontsize = 12, color = 'brown')ax2_twin.tick_params(axis = 'y', labelcolor = 'brown')ax2.set_xlabel('Timestamp', fontsize = 12)ax2.set_ylabel('Rolling Std', fontsize = 12)ax2.set_title('Feature Scores: Rolling Variance and Autocorrelation', fontsize = 14, fontweight = 'bold')ax2.legend(loc = 'upper left')ax2_twin.legend(loc = 'upper right')ax2.grid(True, alpha = 0.3)",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_rolling_features_panel.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_rolling_features_panel.png\").exists(): display(Image(str(output_dir / \"02_rolling_features_panel.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA/UMAP Visualization - Class Separation\n",
    "\n",
    "Visualize feature space in 2D using PCA or UMAP to see class separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features first\n",
    "\n",
    "if not done\n",
    "\n",
    "if 'features_df' not in locals(): print(\"Extracting temporal features...\")\n",
    "\n",
    "features_df = extract_temporal_features(df, id_column = \"id\", timestamp_column = \"timestamp\", window_sizes = [6, 12, 24], autocorr_lags = [1, 6, 12, 24], aggregate_per_id = True,) print(f\"Features extracted: {features_df.shape}\")",
    "\n",
    "# prepare data\n",
    "\n",
    "for PCA / UMAP\n",
    "\n",
    "feature_cols = [c\n",
    "\n",
    "for c in features_df.columns\n",
    "\n",
    "if c not in ['id', 'label']]X = features_df[feature_cols].fillna(0).\n",
    "\n",
    "valuesy = features_df['label'].map({'normal': 0, 'fake': 1}).values\n",
    "\n",
    "# standardize\n",
    "\n",
    "featuresscaler = StandardScaler()X\n",
    "\n",
    "_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA visualization\n",
    "\n",
    "if HAS_PCA:\n",
    "\n",
    "pca = PCA(n_components = 2, random_state = 42) X\n",
    "\n",
    "_pca = pca.fit_transform(X_scaled) fig,\n",
    "\n",
    "axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "\n",
    "# PCA plot\n",
    "\n",
    "normal_mask = y == 0\n",
    "\n",
    "fake_mask = y == 1 axes[0].scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], alpha = 0.6, label = 'Normal', color = 'blue', s = 30) axes[0].scatter(X_pca[fake_mask, 0], X_pca[fake_mask, 1], alpha = 0.6, label = 'Fake', color = 'red', s = 30) axes[0].set_xlabel(f'PC1({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize = 12) axes[0].set_ylabel(f'PC2({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize = 12) axes[0].set_title('PCA Visualization - Feature Space', fontsize = 14, fontweight = 'bold') axes[0].legend() axes[0].grid(True, alpha = 0.3)",
    "\n",
    "# UMAP visualization\n",
    "\n",
    "if HAS_UMAP:\n",
    "\n",
    "reducer = umap.UMAP(n_components = 2, random_state = 42, n_neighbors = 15, min_dist = 0.1) X\n",
    "\n",
    "_umap = reducer.fit_transform(X_scaled) axes[1].scatter(X_umap[normal_mask, 0], X_umap[normal_mask, 1], alpha = 0.6, label = 'Normal', color = 'blue', s = 30) axes[1].scatter(X_umap[fake_mask, 0], X_umap[fake_mask, 1], alpha = 0.6, label = 'Fake', color = 'red', s = 30) axes[1].set_xlabel('UMAP 1', fontsize = 12) axes[1].set_ylabel('UMAP 2', fontsize = 12) axes[1].set_title('UMAP Visualization - Feature Space', fontsize = 14, fontweight = 'bold') axes[1].legend() axes[1].grid(True, alpha = 0.3)",
    "\n",
    "else: axes[1].text(0.5, 0.5, 'UMAP not available', ha = 'center', va = 'center', fontsize = 14) axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_pca_umap_visualization.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_pca_umap_visualization.png\").exists(): display(Image(str(output_dir / \"02_pca_umap_visualization.png\"))) print(f\"\\nPCA explained variance ratio: {pca.explained_variance_ratio_}\") print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.2%}\")",
    "\n",
    "else: print(\"PCA not available. Install sklearn to enable this visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features aggregated per video ID\n",
    "\n",
    "print(\"Extracting temporal features...\")\n",
    "\n",
    "features_df = extract_temporal_features(\n",
    "\n",
    "df,\n",
    "\n",
    "id_column = \"id\",\n",
    "\n",
    "timestamp_column = \"timestamp\",\n",
    "\n",
    "window_sizes = [6, 12, 24],\n",
    "\n",
    "autocorr_lags = [1, 6, 12, 24],\n",
    "\n",
    "aggregate_per_id = True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nFeatures extracted: {features_df.shape}\")\n",
    "\n",
    "print(f\"Number of feature columns: {len([c for c in features_df.columns if c not in ['id', 'label']])}\")\n",
    "\n",
    "print(f\"\\nFeature columns(first 20):\")\n",
    "\n",
    "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
    "\n",
    "print(feature_cols[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Heatmap\n",
    "\n",
    "Visualize feature importance as a heatmap for easy identification of discriminative features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature importance heatmap\n",
    "\n",
    "if 'importance_df' in locals() and len(importance_df) > 0: top_30\n",
    "\n",
    "_features = importance_df.head(30)\n",
    "\n",
    "# prepare data\n",
    "\n",
    "for heatmap\n",
    "\n",
    "heatmap_data = top_30_features[['normal_mean', 'fake_mean', 'effect_size', 'p_value']].copy() heatmap_data[' - log10(p_value)'] =  - \n",
    "\n",
    "np.log10(heatmap_data['p_value'] + 1e - 10)\n",
    "\n",
    "heatmap_data = heatmap_data[['normal_mean', 'fake_mean', 'effect_size', ' - log10(p_value)']] heatmap_data.\n",
    "\n",
    "index = top_30_features['feature'] fig,\n",
    "\n",
    "ax = plt.subplots(1, 1, figsize = (10, max(10, len(top_30_features) * 0.4)))\n",
    "\n",
    "sns.heatmap(heatmap_data.T, annot = False, fmt = '.2f', cmap = 'YlOrRd', cbar_kws = {'label': 'Value'}, ax = ax, linewidths = 0.5) ax.set_title('Top 30 Feature Importance Heatmap', fontsize = 14, fontweight = 'bold') ax.set_xlabel('Feature', fontsize = 12) ax.set_ylabel('Metric', fontsize = 12) ax.set_xticklabels(ax.get_xticklabels(), rotation = ",
    "\n",
    "if HAS_IPYTHON and (output_dir / \"02_feature_engineering_01_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_01_plot.png\")))\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_01_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_01_plot.png\")))\n",
    "\n",
    "# create effect size bar chart fig,\n",
    "\n",
    "ax = plt.subplots(1, 1, figsize = (12, max(8, len(top_30_features) * 0.3)))\n",
    "\n",
    "colors = ['red'\n",
    "\n",
    "if p < 0.05 else 'gray'\n",
    "\n",
    "for p in top_30_features['p_value']] ax.barh(range(len(top_30_features)), top_30_features['effect_size'], color = colors, alpha = 0.7) ax.set_yticks(range(len(top_30_features))) ax.set_yticklabels(top_30_features['feature'], fontsize = 9) ax.set_xlabel('Effect Size', fontsize = 12) ax.set_title('Top 30 Features by Effect Size(R",
    "\n",
    "ed = Signif icant, p<0.05)', fontsize = 14, fontweight = 'bold') ax.grid(True, alpha = 0.3, axis = 'x')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_feature_engineering_02_plot.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_02_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_02_plot.png\")))\n",
    "\n",
    "else: print(\"Feature importance not computed yet. Run the statistical analysis cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entropy, Autocorrelation, and Burst Detection Visualization\n",
    "\n",
    "Visualize entropy plots, autocorrelation plots, and burst detection overlay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy, autocorrelation, and burst detection visualization\n",
    "\n",
    "if 'features_df' not in locals():\n",
    "\n",
    "print(\"Extracting temporal features first...\")\n",
    "\n",
    "features_df = extract_temporal_features(\n",
    "\n",
    "df,\n",
    "\n",
    "id_column = \"id\",\n",
    "\n",
    "timestamp_column = \"timestamp\",\n",
    "\n",
    "window_sizes = [6, 12, 24],\n",
    "\n",
    "autocorr_lags = [1, 6, 12, 24],\n",
    "\n",
    "aggregate_per_id = True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# select a sample series with anomalies if available\n",
    "\n",
    "sample_user_id = df['id'].unique()[0]\n",
    "\n",
    "sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "\n",
    "\n",
    "\n",
    "# compute entropy over tim\n",
    "e(rolling window)\n",
    "\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "\n",
    "\n",
    "\n",
    "def.rolling_entropy(series, window = 12):\n",
    "\n",
    "\"\"\"Compute rolling entropy.\"\"\"\n",
    "\n",
    "entropies = []\n",
    "\n",
    "for i in range(len(series)):\n",
    "\n",
    "start = max(0, i - window // 2)\n",
    "\n",
    "end = min(len(series), i + window // 2)\n",
    "\n",
    "window_data = series[start:\n",
    "end]\n",
    "\n",
    "if len(window_data) > 1:\n",
    "\n",
    "# discretize for entropy calculation\n",
    "\n",
    "hist, _ = np.histogram(window_data, bins = min(10, len(window_data)))\n",
    "\n",
    "hist = hist + 1e - 10 # avoid.lo\n",
    "g(0)\n",
    "\n",
    "hist = hist / hist.sum()\n",
    "\n",
    "ent = scipy_entropy(hist)\n",
    "\n",
    "entropies.append(ent)\n",
    "\n",
    "else:\n",
    "\n",
    "entropies.append(0.0)\n",
    "\n",
    "return np.array(entropies)\n",
    "\n",
    "\n",
    "\n",
    "# compute autocorrelation at dif ferent lags\n",
    "\n",
    "def.compute_rolling_autocorr(series, lag = 1, window = 24):\n",
    "\n",
    "\"\"\"Compute rolling autocorrelation.\"\"\"\n",
    "\n",
    "autocorrs = []\n",
    "\n",
    "for i in range(len(series)):\n",
    "\n",
    "start = max(0, i - window // 2)\n",
    "\n",
    "end = min(len(series), i + window // 2)\n",
    "\n",
    "window_data = series[start:\n",
    "end].values\n",
    "\n",
    "if len(window_data) > lag:\n",
    "\n",
    "if np.std(window_data) > 0:\n",
    "\n",
    "corr = np.corrcoef(window_data[: - lag], window_data[lag:])[0, 1]\n",
    "\n",
    "autocorrs.append(corr if not np.isnan(corr) else 0.0)\n",
    "\n",
    "else:\n",
    "\n",
    "autocorrs.append(0.0)\n",
    "\n",
    "else:\n",
    "\n",
    "autocorrs.append(0.0)\n",
    "\n",
    "return np.array(autocorrs)\n",
    "\n",
    "\n",
    "\n",
    "# detect burst\n",
    "s(peaks)\n",
    "\n",
    "def.detect_bursts_overlay(series, threshold_multiplier = 2.0):\n",
    "\n",
    "\"\"\"Detect bursts and return mask.\"\"\"\n",
    "\n",
    "mean_val = series.mean()\n",
    "\n",
    "std_val = series.std()\n",
    "\n",
    "threshold = mean_val + threshold_multiplier * std_val\n",
    "\n",
    "return series > threshold\n",
    "\n",
    "\n",
    "\n",
    "# compute features\n",
    "\n",
    "entropy_rolling = rolling_entropy(sample_series['views'], window = 12)\n",
    "\n",
    "autocorr_rolling = compute_rolling_autocorr(sample_series['views'], lag = 1, window = 24)\n",
    "\n",
    "burst_mask = detect_bursts_overlay(sample_series['views'], threshold_multiplier = 2.0)\n",
    "\n",
    "\n",
    "\n",
    "# create comprehensive visualization\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize = (16, 14), sharex = True)\n",
    "\n",
    "\n",
    "\n",
    "# Panel 1: Original series with burst overlay\n",
    "\n",
    "ax1 = axes[0]\n",
    "\n",
    "ax1.plot(sample_series['timestamp'], sample_series['views'],\n",
    "\n",
    "label = 'Views', linewidth = 2, color = 'blue', alpha = 0.7)\n",
    "\n",
    "if burst_mask.any():\n",
    "\n",
    "ax1.scatter(sample_series['timestamp'][burst_mask],\n",
    "\n",
    "sample_series['views'].values[burst_mask],\n",
    "\n",
    "marker = 'o', s = 50, color = 'red', label = 'Burst Detection', zorder = 5)\n",
    "\n",
    "ax1.set_ylabel('Views', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax1.set_title(f'Time Series with Burst Detection Overlay - User: {sample_user_id}',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax1.grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Panel 2: Entropy over time\n",
    "\n",
    "ax2 = axes[1]\n",
    "\n",
    "ax2.plot(sample_series['timestamp'], entropy_rolling,\n",
    "\n",
    "label = 'Rolling Entropy', linewidth = 2, color = 'green')\n",
    "\n",
    "ax2.fill_between(sample_series['timestamp'], 0, entropy_rolling,\n",
    "\n",
    "alpha = 0.3, color = 'green')\n",
    "\n",
    "ax2.set_ylabel('Entropy', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax2.set_title('Entropy Over Time(Higher = More Irregular)',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax2.legend()\n",
    "\n",
    "ax2.grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Panel 3: Autocorrelation over time\n",
    "\n",
    "ax3 = axes[2]\n",
    "\n",
    "ax3.plot(sample_series['timestamp'], autocorr_rolling,\n",
    "\n",
    "label = 'Rolling Autocorrelation(lag = 1)', linewidth = 2, color = 'purple')\n",
    "\n",
    "ax3.axhline(y = 0, color = 'gray', linestyle = ' -- ', linewidth = 1)\n",
    "\n",
    "ax3.fill_between(sample_series['timestamp'], 0, autocorr_rolling,\n",
    "\n",
    "alpha = 0.3, color = 'purple', where = (autocorr_rolling > 0))\n",
    "\n",
    "ax3.set_ylabel('Autocorrelation', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax3.set_title('Autocorrelation Over Time(Higher = More Regular Pattern)',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax3.legend()\n",
    "\n",
    "ax3.grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Panel 4: Combined features\n",
    "\n",
    "ax4 = axes[3]\n",
    "\n",
    "ax4_twin = ax4.twinx()\n",
    "\n",
    "line1 = ax4.plot(sample_series['timestamp'], entropy_rolling,\n",
    "\n",
    "label = 'Entropy', linewidth = 2, color = 'green')\n",
    "\n",
    "line2 = ax4_twin.plot(sample_series['timestamp'], autocorr_rolling,\n",
    "\n",
    "label = 'Autocorrelation', linewidth = 2, color = 'purple', linestyle = ' -- ')\n",
    "\n",
    "ax4.set_xlabel('Timestamp', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax4.set_ylabel('Entropy', fontsize = 12, fontweight = 'bold', color = 'green')\n",
    "\n",
    "ax4_twin.set_ylabel('Autocorrelation', fontsize = 12, fontweight = 'bold', color = 'purple')\n",
    "\n",
    "ax4.set_title('Combined Feature View: Entropy and Autocorrelation',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax4.tick_params(axis = 'y', labelcolor = 'green')\n",
    "\n",
    "ax4_twin.tick_params(axis = 'y', labelcolor = 'purple')\n",
    "\n",
    "lines = line1 + line2\n",
    "\n",
    "labels = [l.get_label() for l in lines]\n",
    "\n",
    "ax4.legend(lines, labels, loc = 'upper left')\n",
    "\n",
    "ax4.grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_entropy_autocorr_burst.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_entropy_autocorr_burst.png\").exists():\n",
    "\n",
    "display(Image(str(output_dir / \"02_entropy_autocorr_burst.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zoom on Anomaly Window (1h window)\n",
    "\n",
    "Zoom in on a detected anomaly to see detailed patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom on anomaly windo\n",
    "w(1h window)\n",
    "\n",
    "if 'is_anomaly_window' in df.columns:\n",
    "\n",
    "# find a series with anomalies\n",
    "\n",
    "fake_users = df[df.get('is_fake_series', df.get('label') == 'fake')]['id'].unique()\n",
    "\n",
    "if len(fake_users) > 0:\n",
    "\n",
    "sample_user_id = fake_users[0]\n",
    "\n",
    "sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "\n",
    "anomaly_mask = sample_series.get('is_anomaly_window', pd.Series([False] * len(sample_series))).values\n",
    "\n",
    "\n",
    "\n",
    "if anomaly_mask.any():\n",
    "\n",
    "# find the first anomaly window\n",
    "\n",
    "anomaly_indices = np.where(anomaly_mask)[0]\n",
    "\n",
    "if len(anomaly_indices) > 0:\n",
    "\n",
    "# get a 1 - hour window around the first anomal\n",
    "y(assuming hourly data)\n",
    "\n",
    "center_idx = anomaly_indices[0]\n",
    "\n",
    "window_size = min(12, len(sample_series) // 4) # 12 hours = 1 hour window\n",
    "\n",
    "start_idx = max(0, center_idx - window_size // 2)\n",
    "\n",
    "end_idx = min(len(sample_series), center_idx + window_size // 2)\n",
    "\n",
    "\n",
    "\n",
    "# extract window\n",
    "\n",
    "window_series = sample_series.iloc[start_idx:\n",
    "end_idx]\n",
    "\n",
    "window_anomaly_mask = anomaly_mask[start_idx:\n",
    "end_idx]\n",
    "\n",
    "window_timestamps = window_series['timestamp']\n",
    "\n",
    "window_views = window_series['views']\n",
    "\n",
    "\n",
    "\n",
    "# create zoom visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize = (16, 10), sharex = True)\n",
    "\n",
    "\n",
    "\n",
    "# Top: Full series with zoom region highlighted\n",
    "\n",
    "ax1 = axes[0]\n",
    "\n",
    "ax1.plot(sample_series['timestamp'], sample_series['views'],\n",
    "\n",
    "label = 'Full Series', linewidth = 2, color = 'blue', alpha = 0.5)\n",
    "\n",
    "ax1.axvspan(window_timestamps.iloc[0], window_timestamps.iloc[ - 1],\n",
    "\n",
    "alpha = 0.3, color = 'yellow', label = 'Zoom Region')\n",
    "\n",
    "if anomaly_mask.any():\n",
    "\n",
    "ax1.scatter(sample_series['timestamp'][anomaly_mask],\n",
    "\n",
    "sample_series['views'].values[anomaly_mask],\n",
    "\n",
    "marker = 'o', s = 30, color = 'red', label = 'Anomaly', zorder = 5)\n",
    "\n",
    "ax1.set_ylabel('Views', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax1.set_title(f'Full Series with Zoom Region Highlighted - User: {sample_user_id}',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax1.grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Bottom: Zoomed vie\n",
    "w(1h window)\n",
    "\n",
    "ax2 = axes[1]\n",
    "\n",
    "ax2.plot(window_timestamps, window_views,\n",
    "\n",
    "label = 'Views', linewidth = 3, color = 'blue', marker = 'o', markersize = 4)\n",
    "\n",
    "if window_anomaly_mask.any():\n",
    "\n",
    "ax2.scatter(window_timestamps[window_anomaly_mask],\n",
    "\n",
    "window_views.values[window_anomaly_mask],\n",
    "\n",
    "marker = 'o', s = 100, color = 'red', label = 'Anomaly Window',\n",
    "\n",
    "zorder = 5, edgecolors = 'darkred', linewidths = 2)\n",
    "\n",
    "# add rolling mean for context\n",
    "\n",
    "rolling_mean = window_views.rolling(window = 3, min_periods = 1).mean()\n",
    "\n",
    "ax2.plot(window_timestamps, rolling_mean,\n",
    "\n",
    "label = 'Rolling Mean(3h)', linewidth = 2, color = 'green', linestyle = ' -- ')\n",
    "\n",
    "ax2.fill_between(window_timestamps,\n",
    "\n",
    "rolling_mean - window_views.std(),\n",
    "\n",
    "rolling_mean + window_views.std(),\n",
    "\n",
    "alpha = 0.2, color = 'green', label = 'Std Band')\n",
    "\n",
    "ax2.set_xlabel('Timestamp', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax2.set_ylabel('Views', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax2.set_title(f'Zoomed View: Anomaly Window(1h) - Red Points = Anomaly',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax2.legend()\n",
    "\n",
    "ax2.grid(True, alpha = 0.3)\n",
    "\n",
    "ax2.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_zoom_anomaly_window.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_zoom_anomaly_window.png\").exists():\n",
    "\n",
    "display(Image(str(output_dir / \"02_zoom_anomaly_window.png\")))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nAnomaly Window Statistics:\")\n",
    "\n",
    "print(f\" Window size: {len(window_series)} timesteps\")\n",
    "\n",
    "print(f\" Anomaly points in window: {window_anomaly_mask.sum()}\")\n",
    "\n",
    "print(f\" Mean views in window: {window_views.mean():.2f}\")\n",
    "\n",
    "print(f\" Std views in window: {window_views.std():.2f}\")\n",
    "\n",
    "print(f\" Max views in window: {window_views.max()}\")\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"No anomalies found in sample series.\")\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"No fake users found in dataset.\")\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"is_anomaly_window column not found. Using burst detection instead.\")\n",
    "\n",
    "# fallback: use burst detection\n",
    "\n",
    "sample_user_id = df['id'].unique()[0]\n",
    "\n",
    "sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "\n",
    "burst_mask = detect_bursts_overlay(sample_series['views'], threshold_multiplier = 2.5)\n",
    "\n",
    "\n",
    "\n",
    "if burst_mask.any():\n",
    "\n",
    "center_idx = np.where(burst_mask)[0][0]\n",
    "\n",
    "window_size = 12\n",
    "\n",
    "start_idx = max(0, center_idx - window_size // 2)\n",
    "\n",
    "end_idx = min(len(sample_series), center_idx + window_size // 2)\n",
    "\n",
    "\n",
    "\n",
    "window_series = sample_series.iloc[start_idx:\n",
    "end_idx]\n",
    "\n",
    "window_burst_mask = burst_mask[start_idx:\n",
    "end_idx]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (16, 6))\n",
    "\n",
    "ax.plot(window_series['timestamp'], window_series['views'],\n",
    "\n",
    "label = 'Views', linewidth = 3, color = 'blue', marker = 'o', markersize = 4)\n",
    "\n",
    "ax.scatter(window_series['timestamp'][window_burst_mask],\n",
    "\n",
    "window_series['views'].values[window_burst_mask],\n",
    "\n",
    "marker = 'o', s = 100, color = 'red', label = 'Burst Detection',\n",
    "\n",
    "zorder = 5, edgecolors = 'darkred', linewidths = 2)\n",
    "\n",
    "ax.set_xlabel('Timestamp', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax.set_ylabel('Views', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "ax.set_title(f'Zoomed View: Burst Detection Window - User: {sample_user_id}',\n",
    "\n",
    "fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.grid(True, alpha = 0.3)\n",
    "\n",
    "ax.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_zoom_burst_window.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_zoom_burst_window.png\").exists():\n",
    "\n",
    "display(Image(str(output_dir / \"02_zoom_burst_window.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statisticsprin\n",
    "t(\"Feature statistics by label:\n",
    "n\")\n",
    "\n",
    "feature_cols = [c\n",
    "\n",
    "for c in features_df.columns\n",
    "\n",
    "if c not in ['id', 'label']]\n",
    "\n",
    "# select a subset of key features\n",
    "\n",
    "for\n",
    "\n",
    "displaykey_features = [ col\n",
    "\n",
    "for col in feature_cols\n",
    "\n",
    "if any(x in col\n",
    "\n",
    "for x in ['max_mean_ratio', 'n_peaks', 'entropy', 'regularity', 'autocorr_lag_1', 'ratio_likes_views'])]print(f\"Key features statistics:\")\n",
    "print(features_df.groupby('label')[key_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distributions - Normal vs Fake\n",
    "\n",
    "Compare feature distributions between normal and fake engagement patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select key features\n",
    "\n",
    "for\n",
    "\n",
    "visualizationkey_features_viz = [ 'views_max_mean_ratio', 'views_n_peaks', 'views_entropy', 'views_regularity', 'views_autocorr_lag_1', 'ratio_likes_views', 'likes_max_mean_ratio', 'likes_n_peaks',]\n",
    "\n",
    "# filter to available\n",
    "\n",
    "featureskey_features_viz = [f\n",
    "\n",
    "for f in key_features_viz\n",
    "\n",
    "if f in features_df.columns]\n",
    "\n",
    "# plot\n",
    "\n",
    "distributionsn_features = len(key_features_viz)\n",
    "\n",
    "n_cols = 3\n",
    "\n",
    "n_rows = (n_features + n_cols - 1) // n_colsfig,\n",
    "\n",
    "axes = plt.subplots(n_rows, n_cols, figsize = (15, 5 * n_rows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(key_features_viz):\n",
    "\n",
    "ax = axes[idx]\n",
    "\n",
    "normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_data = features_df[features_df['label'] == 'fake'][feature].dropna() ax.hist(normal_data, bins = 30, alpha = 0.6, label = 'Normal', color = 'blue', density = True) ax.hist(fake_data, bins = 30, alpha = 0.6, label = 'Fake', color = 'red', density = True) ax.set_xlabel(feature.replace('_', ' ').title(), fontsize = 10) ax.set_ylabel('Density', fontsize = 10) ax.set_title(f'Distribution: {feature}', fontsize = 12, fontweight = 'bold') ax.legend() ax.grid(True, alpha = 0.3)",
    "\n",
    "# hide extra subplots\n",
    "\n",
    "for idx in range(n_features, len(axes)): axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_feature_engineering_03_plot.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_03_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_03_plot.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats\n",
    "\n",
    "import mannwhitneyu\n",
    "\n",
    "# compute statistical signif icance\n",
    "\n",
    "for each\n",
    "\n",
    "featurefeature_importance = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "\n",
    "normal_values = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_values = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "\n",
    "if len(normal_values) > 0 and len(fake_values) > 0:\n",
    "\n",
    "# mann - whitney U test\n",
    "\n",
    "try: stat,\n",
    "\n",
    "p_value = mannwhitneyu(normal_values, fake_values, alternative = 'two - sided')\n",
    "\n",
    "# effect siz\n",
    "e(dif ference in means normalized by pooled std)\n",
    "\n",
    "mean_dif f = fake_values.mean() - normal_values.mean()\n",
    "\n",
    "pooled_std = np.sqrt((normal_values.std() ** 2 + fake_values.std() ** 2) / 2)\n",
    "\n",
    "effect_size = mean_d",
    "if f / (pooled_std + 1e - 6) feature_",
    "importance.append({ 'feature': feature, 'normal_mean': normal_values.mean(), 'fake_mean': fake_values.mean(), 'mean_d",
    "if f': mean_d",
    "if f, 'effect_size': abs(effect_size), 'p_value': p_value, 'sign",
    "if icant': p_value < 0.05, }) except:",
    "\n",
    "passimportance_df = pd.DataFrame(feature_importance).sort_values('effect_size', ascending = False)\n",
    "print(\"Top 20 most discriminative features:\")\n",
    "print(importance_df.head(20)[['feature', 'normal_mean', 'fake_mean', 'effect_size', 'p_value', 'signif icant']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Distributions Analysis\n",
    "\n",
    "Visualize distributions of key features: variance, entropy, and burst features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select key features\n",
    "\n",
    "for distribution\n",
    "\n",
    "analysisvariance_features = [f\n",
    "\n",
    "for f in feature_cols\n",
    "\n",
    "if 'rolling_std' in f or 'variance' in f][:4]\n",
    "\n",
    "entropy_features = [f\n",
    "\n",
    "for f in feature_cols\n",
    "\n",
    "if 'entropy' in f][:4]\n",
    "\n",
    "burst_features = [f\n",
    "\n",
    "for f in feature_cols\n",
    "\n",
    "if 'peaks' in f or 'max_mean' in f][:4]\n",
    "\n",
    "# create distribution plotsfig,\n",
    "\n",
    "axes = plt.subplots(3, 4, figsize = (18, 12))\n",
    "\n",
    "# variance features\n",
    "\n",
    "for idx, feature in enumerate(variance_features):\n",
    "\n",
    "if idx < 4:\n",
    "\n",
    "ax = axes[0, idx]\n",
    "\n",
    "normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_data = features_df[features_df['label'] == 'fake'][feature].dropna() ax.hist(normal_data, bins = 30, alpha = 0.6, label = 'Normal', color = 'blue', density = True) ax.hist(fake_data, bins = 30, alpha = 0.6, label = 'Fake', color = 'red', density = True) ax.set_xlabel(feature.replace('_', ' ').title(), fontsize = 10) ax.set_ylabel('Density', fontsize = 10) ax.set_title(f'Variance Feature: {feature.split(\"_\")[ - 1]}', fontsize = 11, fontweight = 'bold') ax.legend() ax.grid(True, alpha = 0.3)",
    "\n",
    "# entropy features\n",
    "\n",
    "for idx, feature in enumerate(entropy_features):\n",
    "\n",
    "if idx < 4:\n",
    "\n",
    "ax = axes[1, idx]\n",
    "\n",
    "normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_data = features_df[features_df['label'] == 'fake'][feature].dropna() ax.hist(normal_data, bins = 30, alpha = 0.6, label = 'Normal', color = 'blue', density = True) ax.hist(fake_data, bins = 30, alpha = 0.6, label = 'Fake', color = 'red', density = True) ax.set_xlabel(feature.replace('_', ' ').title(), fontsize = 10) ax.set_ylabel('Density', fontsize = 10) ax.set_title(f'Entropy Feature: {feature.split(\"_\")[0]}', fontsize = 11, fontweight = 'bold') ax.legend() ax.grid(True, alpha = 0.3)",
    "\n",
    "# burst features\n",
    "\n",
    "for idx, feature in enumerate(burst_features):\n",
    "\n",
    "if idx < 4:\n",
    "\n",
    "ax = axes[2, idx]\n",
    "\n",
    "normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_data = features_df[features_df['label'] == 'fake'][feature].dropna() ax.hist(normal_data, bins = 30, alpha = 0.6, label = 'Normal', color = 'blue', density = True) ax.hist(fake_data, bins = 30, alpha = 0.6, label = 'Fake', color = 'red', density = True) ax.set_xlabel(feature.replace('_', ' ').title(), fontsize = 10) ax.set_ylabel('Density', fontsize = 10) ax.set_title(f'Burst Feature: {feature.split(\"_\")[ - 1]}', fontsize = 11, fontweight = 'bold') ax.legend() ax.grid(True, alpha = 0.3)",
    "\n",
    "plt.suptitle('Feature Distributions: Variance, Entropy, and Burst Features', fontsize = 16, fontweight = 'bold', y = 0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_feature_engineering_04_plot.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_04_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_04_plot.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Before/After Normalization Visualization\n",
    "\n",
    "Compare feature distributions before and after normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features without\n",
    "\n",
    "normalizationfeatures_before = extract_temporal_features(df, id_column = \"id\", timestamp_column = \"timestamp\", aggregate_per_id = True, normalize = False)\n",
    "\n",
    "# extract features with normalizatio\n",
    "n(standardize)\n",
    "\n",
    "from src.data.preprocess\n",
    "\n",
    "import\n",
    "\n",
    "normalize_engagement_metricsdf_normalized = df.copy()df_normalized[['views', 'likes', 'comments', 'shares']] = normalize_engagement_metrics(df_normalized[['views', 'likes', 'comments', 'shares']], method = 'standardize')",
    "\n",
    "features_after = extract_temporal_features(df_normalized, id_column = \"id\", timestamp_column = \"timestamp\", aggregate_per_id = True, normalize = False)\n",
    "\n",
    "# select a few key features\n",
    "\n",
    "for\n",
    "\n",
    "comparisonkey_features = ['views_rolling_mean_6', 'views_rolling_std_6', 'views_entropy', 'views_n_peaks']\n",
    "\n",
    "key_features = [f\n",
    "\n",
    "for f in key_features\n",
    "\n",
    "if f in features_before.columns and f in features_after.columns]\n",
    "\n",
    "# create before / after comparison plotsfig,\n",
    "\n",
    "axes = plt.subplots(2, len(key_features), figsize = (5 * len(key_features), 10))\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "\n",
    "# before normalization\n",
    "\n",
    "ax_before = axes[0, idx]\n",
    "\n",
    "normal_before = features_before[features_before['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_before = features_before[features_before['label'] == 'fake'][feature].dropna() ax_before.hist(normal_before, bins = 30, alpha = 0.6, label = 'Normal', color = 'blue', density = True) ax_before.hist(fake_before, bins = 30, alpha = 0.6, label = 'Fake', color = 'red', density = True) ax_before.set_xlabel('Value', fontsize = 10) ax_before.set_ylabel('Density', fontsize = 10) ax_before.set_title(f'Before Normalization\\\\n{feature}', fontsize = 11, fontweight = 'bold') ax_before.legend() ax_before.grid(True, alpha = 0.3)",
    "\n",
    "# after normalization\n",
    "\n",
    "ax_after = axes[1, idx]\n",
    "\n",
    "normal_after = features_after[features_after['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_after = features_after[features_after['label'] == 'fake'][feature].dropna() ax_after.hist(normal_after, bins = 30, alpha = 0.6, label = 'Normal', color = 'blue', density = True) ax_after.hist(fake_after, bins = 30, alpha = 0.6, label = 'Fake', color = 'red', density = True) ax_after.set_xlabel('Value', fontsize = 10) ax_after.set_ylabel('Density', fontsize = 10) ax_after.set_title(f'After Normalization\\\\n{feature}', fontsize = 11, fontweight = 'bold') ax_after.legend() ax_after.grid(True, alpha = 0.3)",
    "\n",
    "plt.suptitle('Feature Distributions: Before vs After Normalization', fontsize = 16, fontweight = 'bold', y = 0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_feature_engineering_05_plot.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_05_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_05_plot.png\")))\n",
    "\n",
    "# print statisticsprin\n",
    "t(\" = \" * 60)\n",
    "print(\"NORMALIZATION IMPACT\")\n",
    "print(\" = \" * 60)\n",
    "\n",
    "for feature in key_features: print(f\"\\\\n{feature}:\") print(f\" Before - Normal mean: {features_before[features_before['label'] == 'normal'][feature].mean():.4f}, std: {features_before[features_before['label'] == 'normal'][feature].std():.4f}\") print(f\" Before - Fake mean: {features_before[features_before['label'] == 'fake'][feature].mean():.4f}, std: {features_before[features_before['label'] == 'fake'][feature].std():.4f}\") print(f\" After - Normal mean: {features_after[features_after['label'] == 'normal'][feature].mean():.4f}, std: {features_after[features_after['label'] == 'normal'][feature].std():.4f}\") print(f\" After - Fake mean: {features_after[features_after['label'] == 'fake'][feature].mean():.4f}, std: {features_after[features_after['label'] == 'fake'][feature].std():.4f}\")",
    "print(\" = \" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 9\n",
    "\n",
    "featurestop_features = importance_df.head(9)['feature'].tolist()fig,\n",
    "\n",
    "axes = plt.subplots(3, 3, figsize = (18, 15))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "\n",
    "ax = axes[idx]\n",
    "\n",
    "normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "\n",
    "fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "\n",
    "# box plot\n",
    "\n",
    "bp = ax.boxplot([normal_data, fake_data], labels = ['Normal', 'Fake'], patch_artist = True) bp['boxes'][0].set_facecolor('blue') bp['boxes'][0].set_alpha(0.6) bp['boxes'][1].set_facecolor('red') bp['boxes'][1].set_alpha(0.6) ax.set_ylabel(feature.replace('_', ' ').title(), fontsize = 10) ax.set_title(f'Top Feature #{idx + 1}', fontsize = 12, fontweight = 'bold') ax.grid(True, alpha = 0.3, axis = 'y')",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_feature_engineering_06_plot.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_06_plot.png\").exists(): display(Image(str(output_dir / \"02_feature_engineering_06_plot.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: Feature Extraction for Single Video\n",
    "\n",
    "Demonstrate feature extraction for a single video to understand the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one normal and one fake video\n",
    "\n",
    "normal_id = df[df['label'] == 'normal']['id'].iloc[0]\n",
    "\n",
    "fake_id = df[df['label'] == 'fake']['id'].iloc[0]\n",
    "\n",
    "normal_video = df[df['id'] == normal_id].sort_values('timestamp')\n",
    "\n",
    "fake_video = df[df['id'] == fake_id].sort_values('timestamp')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Normal video: {normal_id}\")\n",
    "\n",
    "print(f\"Fake video: {fake_id}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# compute features for views metric\n",
    "\n",
    "print(\" ==  = Rolling Statistics(views) ==  = \")\n",
    "\n",
    "normal_rolling = compute_rolling_statistics(normal_video['views'], window_sizes = [6, 12, 24])\n",
    "\n",
    "print(\"Normal video - sample rolling features:\")\n",
    "\n",
    "print(normal_rolling.head(10))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ==  = Burst Detection(views) ==  = \")\n",
    "\n",
    "normal_bursts = detect_bursts(normal_video['views'])\n",
    "\n",
    "fake_bursts = detect_bursts(fake_video['views'])\n",
    "\n",
    "print(f\"Normal: {normal_bursts}\")\n",
    "\n",
    "print(f\"Fake: {fake_bursts}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ==  = Autocorrelation(views) ==  = \")\n",
    "\n",
    "normal_autocorr = compute_autocorrelation(normal_video['views'], lags = [1, 6, 12, 24])\n",
    "\n",
    "fake_autocorr = compute_autocorrelation(fake_video['views'], lags = [1, 6, 12, 24])\n",
    "\n",
    "print(f\"Normal: {normal_autocorr}\")\n",
    "\n",
    "print(f\"Fake: {fake_autocorr}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ==  = Entropy(views) ==  = \")\n",
    "\n",
    "normal_entropy = compute_entropy(normal_video['views'])\n",
    "\n",
    "fake_entropy = compute_entropy(fake_video['views'])\n",
    "\n",
    "print(f\"Normal: {normal_entropy}\")\n",
    "\n",
    "print(f\"Fake: {fake_entropy}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ==  = Ratios ==  = \")\n",
    "\n",
    "normal_ratios = compute_ratios(normal_video, numerator_cols = ['likes', 'comments'], denominator_col = 'views')\n",
    "\n",
    "print(\"Normal video - sample ratios:\")\n",
    "\n",
    "print(normal_ratios.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Features\n",
    "\n",
    "Save the extracted features to disk for use in modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "\n",
    "output_path = project_root / \"data\" / \"processed\" / \"temporal_features.parquet\"\n",
    "\n",
    "save_features(\n",
    "\n",
    "features_df,\n",
    "\n",
    "output_path = str(output_path),\n",
    "\n",
    "output_format = \"parquet\",\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nFeatures saved successfully!\")\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"Total videos: {len(features_df)}\")\n",
    "\n",
    "print(f\"Normal videos: {len(features_df[features_df['label'] == 'normal'])}\")\n",
    "\n",
    "print(f\"Fake videos: {len(features_df[features_df['label'] == 'fake'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Correlation Analysis\n",
    "\n",
    "Analyze correlations between features to identify redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrix for top features\n",
    "\n",
    "top_20_features = importance_df.head(20)['feature'].tolist()\n",
    "\n",
    "top_20_features = [f for f in top_20_features if f in features_df.columns]\n",
    "\n",
    "\n",
    "\n",
    "corr_matrix = features_df[top_20_features].corr()\n",
    "\n",
    "\n",
    "\n",
    "# plot correlation heatmap\n",
    "\n",
    "plt.figure(figsize = (14, 12))\n",
    "\n",
    "sns.heatmap(\n",
    "\n",
    "corr_matrix,\n",
    "\n",
    "annot = False,\n",
    "\n",
    "cmap = 'coolwarm',\n",
    "\n",
    "center = 0,\n",
    "\n",
    "vmin =  - 1,\n",
    "\n",
    "vmax = 1,\n",
    "\n",
    "square = True,\n",
    "\n",
    "fmt = '.2f',\n",
    "\n",
    ")\n",
    "\n",
    "plt.title('Feature Correlation Matrix(Top 20 Features)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_dir / \"02_feature_engineering_07_plot.png\", dpi = 150, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if HAS_IPYTHON and(output_dir / \"02_feature_engineering_07_plot.png\").exists():\n",
    "\n",
    "display(Image(str(output_dir / \"02_feature_engineering_07_plot.png\")))\n",
    "\n",
    "\n",
    "\n",
    "# find highly correlated feature pairs\n",
    "\n",
    "print(\"\\nHighly correlated feature pairs(|correlation| > 0.8):\")\n",
    "\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "\n",
    "for j in range(i + 1, len(corr_matrix.columns)):\n",
    "\n",
    "corr_val = corr_matrix.iloc[i, j]\n",
    "\n",
    "if abs(corr_val) > 0.8:\n",
    "\n",
    "high_corr_pairs.append((\n",
    "\n",
    "corr_matrix.columns[i],\n",
    "\n",
    "corr_matrix.columns[j],\n",
    "\n",
    "corr_val\n",
    "\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "if high_corr_pairs:\n",
    "\n",
    "for feat1, feat2, corr in high_corr_pairs[:10]:\n",
    "\n",
    "print(f\"{feat1} < - > {feat2}: {corr:.3f}\")\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"No highly correlated pairs found(threshold: 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Summary of feature engineering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" = \" * 60)\n",
    "\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "\n",
    "print(\" = \" * 60)\n",
    "\n",
    "print(f\"\\n1. Total features extracted: {len(feature_cols)}\")\n",
    "\n",
    "print(f\" - Rolling statistics: {len([f for f in feature_cols if 'rolling' in f])}\")\n",
    "\n",
    "print(f\" - Burst features: {len([f for f in feature_cols if 'peaks' in f or 'max_mean' in f])}\")\n",
    "\n",
    "print(f\" - Autocorrelation: {len([f for f in feature_cols if 'autocorr' in f])}\")\n",
    "\n",
    "print(f\" - Entropy / Regularity: {len([f for f in feature_cols if 'entropy' in f or 'regularity' in f])}\")\n",
    "\n",
    "print(f\" - Ratio features: {len([f for f in feature_cols if 'ratio' in f])}\")\n",
    "\n",
    "print(f\" - Trend features: {len([f for f in feature_cols if 'trend' in f])}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n2. Statistical signif icance:\")\n",
    "\n",
    "signif icant_features = importance_df[importance_df['signif icant'] == True]\n",
    "\n",
    "print(f\" - Signif icant features(p < 0.05): {len(signif icant_features)}\")\n",
    "\n",
    "print(f\" - Top 5 most discriminative:\")\n",
    "\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "\n",
    "print(f\" {row['feature']}: effect_size = {row['effect_size']:.3f}, p = {row['p_value']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n3. Feature quality:\")\n",
    "\n",
    "print(f\" - Features saved to: {output_path}\")\n",
    "\n",
    "print(f\" - Ready for baseline models(tree - based, anomaly detection)\")\n",
    "\n",
    "print(f\" - Ready for deep learning models(as additional features)\")\n",
    "\n",
    "print(\" = \" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}