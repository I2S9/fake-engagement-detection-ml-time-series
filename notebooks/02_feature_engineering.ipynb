{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering - Temporal Features\n",
        "\n",
        "This notebook extracts and analyzes temporal features from engagement time series data.\n",
        "\n",
        "Features include:\n",
        "- Rolling statistics (mean, std, min, max)\n",
        "- Ratio features (likes/views, comments/views)\n",
        "- Burst detection (peaks, max/mean ratio)\n",
        "- Autocorrelation at different lags\n",
        "- Entropy and regularity measures\n",
        "- Trend features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# set plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# import project modules\n",
        "from src.data.preprocess import load_and_preprocess\n",
        "from src.features.temporal_features import (\n",
        "    extract_temporal_features,\n",
        "    save_features,\n",
        "    compute_rolling_statistics,\n",
        "    compute_ratios,\n",
        "    detect_bursts,\n",
        "    compute_autocorrelation,\n",
        "    compute_entropy,\n",
        "    compute_trend_features,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Preprocessed Data\n",
        "\n",
        "Load the preprocessed time series data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load preprocessed data\n",
        "data_path = project_root / \"data\" / \"raw\" / \"engagement_timeseries.parquet\"\n",
        "\n",
        "df = load_and_preprocess(\n",
        "    file_path=str(data_path),\n",
        "    target_timezone=\"UTC\",\n",
        "    resample_frequency=\"h\",\n",
        "    handle_missing=True,\n",
        "    missing_method=\"forward\",\n",
        "    normalize=False,\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of unique videos: {df['id'].nunique()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract Temporal Features\n",
        "\n",
        "Extract all temporal features for each video ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract features aggregated per video ID\n",
        "print(\"Extracting temporal features...\")\n",
        "features_df = extract_temporal_features(\n",
        "    df,\n",
        "    id_column=\"id\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    window_sizes=[6, 12, 24],\n",
        "    autocorr_lags=[1, 6, 12, 24],\n",
        "    aggregate_per_id=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nFeatures extracted: {features_df.shape}\")\n",
        "print(f\"Number of feature columns: {len([c for c in features_df.columns if c not in ['id', 'label']])}\")\n",
        "print(f\"\\nFeature columns (first 20):\")\n",
        "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
        "print(feature_cols[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Statistics\n",
        "\n",
        "Analyze the distribution and statistics of extracted features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# basic statistics\n",
        "print(\"Feature statistics by label:\\n\")\n",
        "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
        "\n",
        "# select a subset of key features for display\n",
        "key_features = [\n",
        "    col for col in feature_cols \n",
        "    if any(x in col for x in ['max_mean_ratio', 'n_peaks', 'entropy', 'regularity', 'autocorr_lag_1', 'ratio_likes_views'])\n",
        "]\n",
        "\n",
        "print(f\"Key features statistics:\")\n",
        "print(features_df.groupby('label')[key_features].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Distributions - Normal vs Fake\n",
        "\n",
        "Compare feature distributions between normal and fake engagement patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select key features for visualization\n",
        "key_features_viz = [\n",
        "    'views_max_mean_ratio',\n",
        "    'views_n_peaks',\n",
        "    'views_entropy',\n",
        "    'views_regularity',\n",
        "    'views_autocorr_lag_1',\n",
        "    'ratio_likes_views',\n",
        "    'likes_max_mean_ratio',\n",
        "    'likes_n_peaks',\n",
        "]\n",
        "\n",
        "# filter to available features\n",
        "key_features_viz = [f for f in key_features_viz if f in features_df.columns]\n",
        "\n",
        "# plot distributions\n",
        "n_features = len(key_features_viz)\n",
        "n_cols = 3\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "axes = axes.flatten() if n_features > 1 else [axes]\n",
        "\n",
        "for idx, feature in enumerate(key_features_viz):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
        "    fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
        "    \n",
        "    ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "    ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
        "    \n",
        "    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
        "    ax.set_ylabel('Density', fontsize=10)\n",
        "    ax.set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# hide extra subplots\n",
        "for idx in range(n_features, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# compute statistical significance for each feature\n",
        "feature_importance = []\n",
        "\n",
        "for feature in feature_cols:\n",
        "    normal_values = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
        "    fake_values = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
        "    \n",
        "    if len(normal_values) > 0 and len(fake_values) > 0:\n",
        "        # mann-whitney U test\n",
        "        try:\n",
        "            stat, p_value = mannwhitneyu(normal_values, fake_values, alternative='two-sided')\n",
        "            \n",
        "            # effect size (difference in means normalized by pooled std)\n",
        "            mean_diff = fake_values.mean() - normal_values.mean()\n",
        "            pooled_std = np.sqrt((normal_values.std()**2 + fake_values.std()**2) / 2)\n",
        "            effect_size = mean_diff / (pooled_std + 1e-6)\n",
        "            \n",
        "            feature_importance.append({\n",
        "                'feature': feature,\n",
        "                'normal_mean': normal_values.mean(),\n",
        "                'fake_mean': fake_values.mean(),\n",
        "                'mean_diff': mean_diff,\n",
        "                'effect_size': abs(effect_size),\n",
        "                'p_value': p_value,\n",
        "                'significant': p_value < 0.05,\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "importance_df = pd.DataFrame(feature_importance).sort_values('effect_size', ascending=False)\n",
        "\n",
        "print(\"Top 20 most discriminative features:\")\n",
        "print(importance_df.head(20)[['feature', 'normal_mean', 'fake_mean', 'effect_size', 'p_value', 'significant']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Distributions Analysis\n",
        "\n",
        "Visualize distributions of key features: variance, entropy, and burst features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select key features for distribution analysis\n",
        "variance_features = [f for f in feature_cols if 'rolling_std' in f or 'variance' in f][:4]\n",
        "entropy_features = [f for f in feature_cols if 'entropy' in f][:4]\n",
        "burst_features = [f for f in feature_cols if 'peaks' in f or 'max_mean' in f][:4]\n",
        "\n",
        "# create distribution plots\n",
        "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
        "\n",
        "# variance features\n",
        "for idx, feature in enumerate(variance_features):\n",
        "    if idx < 4:\n",
        "        ax = axes[0, idx]\n",
        "        normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
        "        fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
        "        \n",
        "        ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "        ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
        "        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
        "        ax.set_ylabel('Density', fontsize=10)\n",
        "        ax.set_title(f'Variance Feature: {feature.split(\"_\")[-1]}', fontsize=11, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "# entropy features\n",
        "for idx, feature in enumerate(entropy_features):\n",
        "    if idx < 4:\n",
        "        ax = axes[1, idx]\n",
        "        normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
        "        fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
        "        \n",
        "        ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "        ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
        "        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
        "        ax.set_ylabel('Density', fontsize=10)\n",
        "        ax.set_title(f'Entropy Feature: {feature.split(\"_\")[0]}', fontsize=11, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "# burst features\n",
        "for idx, feature in enumerate(burst_features):\n",
        "    if idx < 4:\n",
        "        ax = axes[2, idx]\n",
        "        normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
        "        fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
        "        \n",
        "        ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "        ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
        "        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
        "        ax.set_ylabel('Density', fontsize=10)\n",
        "        ax.set_title(f'Burst Feature: {feature.split(\"_\")[-1]}', fontsize=11, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Feature Distributions: Variance, Entropy, and Burst Features', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Before/After Normalization Visualization\n",
        "\n",
        "Compare feature distributions before and after normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract features without normalization\n",
        "features_before = extract_temporal_features(\n",
        "    df,\n",
        "    id_column=\"id\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    aggregate_per_id=True,\n",
        "    normalize=False\n",
        ")\n",
        "\n",
        "# extract features with normalization (standardize)\n",
        "from src.data.preprocess import normalize_engagement_metrics\n",
        "df_normalized = df.copy()\n",
        "df_normalized[['views', 'likes', 'comments', 'shares']] = normalize_engagement_metrics(\n",
        "    df_normalized[['views', 'likes', 'comments', 'shares']],\n",
        "    method='standardize'\n",
        ")\n",
        "\n",
        "features_after = extract_temporal_features(\n",
        "    df_normalized,\n",
        "    id_column=\"id\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    aggregate_per_id=True,\n",
        "    normalize=False\n",
        ")\n",
        "\n",
        "# select a few key features for comparison\n",
        "key_features = ['views_rolling_mean_6', 'views_rolling_std_6', 'views_entropy', 'views_n_peaks']\n",
        "key_features = [f for f in key_features if f in features_before.columns and f in features_after.columns]\n",
        "\n",
        "# create before/after comparison plots\n",
        "fig, axes = plt.subplots(2, len(key_features), figsize=(5 * len(key_features), 10))\n",
        "\n",
        "for idx, feature in enumerate(key_features):\n",
        "    # before normalization\n",
        "    ax_before = axes[0, idx]\n",
        "    normal_before = features_before[features_before['label'] == 'normal'][feature].dropna()\n",
        "    fake_before = features_before[features_before['label'] == 'fake'][feature].dropna()\n",
        "    \n",
        "    ax_before.hist(normal_before, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "    ax_before.hist(fake_before, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
        "    ax_before.set_xlabel('Value', fontsize=10)\n",
        "    ax_before.set_ylabel('Density', fontsize=10)\n",
        "    ax_before.set_title(f'Before Normalization\\\\n{feature}', fontsize=11, fontweight='bold')\n",
        "    ax_before.legend()\n",
        "    ax_before.grid(True, alpha=0.3)\n",
        "    \n",
        "    # after normalization\n",
        "    ax_after = axes[1, idx]\n",
        "    normal_after = features_after[features_after['label'] == 'normal'][feature].dropna()\n",
        "    fake_after = features_after[features_after['label'] == 'fake'][feature].dropna()\n",
        "    \n",
        "    ax_after.hist(normal_after, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "    ax_after.hist(fake_after, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
        "    ax_after.set_xlabel('Value', fontsize=10)\n",
        "    ax_after.set_ylabel('Density', fontsize=10)\n",
        "    ax_after.set_title(f'After Normalization\\\\n{feature}', fontsize=11, fontweight='bold')\n",
        "    ax_after.legend()\n",
        "    ax_after.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Feature Distributions: Before vs After Normalization', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print statistics\n",
        "print(\"=\" * 60)\n",
        "print(\"NORMALIZATION IMPACT\")\n",
        "print(\"=\" * 60)\n",
        "for feature in key_features:\n",
        "    print(f\"\\\\n{feature}:\")\n",
        "    print(f\"  Before - Normal mean: {features_before[features_before['label'] == 'normal'][feature].mean():.4f}, std: {features_before[features_before['label'] == 'normal'][feature].std():.4f}\")\n",
        "    print(f\"  Before - Fake mean: {features_before[features_before['label'] == 'fake'][feature].mean():.4f}, std: {features_before[features_before['label'] == 'fake'][feature].std():.4f}\")\n",
        "    print(f\"  After - Normal mean: {features_after[features_after['label'] == 'normal'][feature].mean():.4f}, std: {features_after[features_after['label'] == 'normal'][feature].std():.4f}\")\n",
        "    print(f\"  After - Fake mean: {features_after[features_after['label'] == 'fake'][feature].mean():.4f}, std: {features_after[features_after['label'] == 'fake'][feature].std():.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get top 9 features\n",
        "top_features = importance_df.head(9)['feature'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(top_features):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
        "    fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
        "    \n",
        "    # box plot\n",
        "    bp = ax.boxplot([normal_data, fake_data], labels=['Normal', 'Fake'], patch_artist=True)\n",
        "    bp['boxes'][0].set_facecolor('blue')\n",
        "    bp['boxes'][0].set_alpha(0.6)\n",
        "    bp['boxes'][1].set_facecolor('red')\n",
        "    bp['boxes'][1].set_alpha(0.6)\n",
        "    \n",
        "    ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=10)\n",
        "    ax.set_title(f'Top Feature #{idx+1}', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Example: Feature Extraction for Single Video\n",
        "\n",
        "Demonstrate feature extraction for a single video to understand the process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select one normal and one fake video\n",
        "normal_id = df[df['label'] == 'normal']['id'].iloc[0]\n",
        "fake_id = df[df['label'] == 'fake']['id'].iloc[0]\n",
        "\n",
        "normal_video = df[df['id'] == normal_id].sort_values('timestamp')\n",
        "fake_video = df[df['id'] == fake_id].sort_values('timestamp')\n",
        "\n",
        "print(f\"Normal video: {normal_id}\")\n",
        "print(f\"Fake video: {fake_id}\\n\")\n",
        "\n",
        "# compute features for views metric\n",
        "print(\"=== Rolling Statistics (views) ===\")\n",
        "normal_rolling = compute_rolling_statistics(normal_video['views'], window_sizes=[6, 12, 24])\n",
        "print(\"Normal video - sample rolling features:\")\n",
        "print(normal_rolling.head(10))\n",
        "\n",
        "print(\"\\n=== Burst Detection (views) ===\")\n",
        "normal_bursts = detect_bursts(normal_video['views'])\n",
        "fake_bursts = detect_bursts(fake_video['views'])\n",
        "print(f\"Normal: {normal_bursts}\")\n",
        "print(f\"Fake: {fake_bursts}\")\n",
        "\n",
        "print(\"\\n=== Autocorrelation (views) ===\")\n",
        "normal_autocorr = compute_autocorrelation(normal_video['views'], lags=[1, 6, 12, 24])\n",
        "fake_autocorr = compute_autocorrelation(fake_video['views'], lags=[1, 6, 12, 24])\n",
        "print(f\"Normal: {normal_autocorr}\")\n",
        "print(f\"Fake: {fake_autocorr}\")\n",
        "\n",
        "print(\"\\n=== Entropy (views) ===\")\n",
        "normal_entropy = compute_entropy(normal_video['views'])\n",
        "fake_entropy = compute_entropy(fake_video['views'])\n",
        "print(f\"Normal: {normal_entropy}\")\n",
        "print(f\"Fake: {fake_entropy}\")\n",
        "\n",
        "print(\"\\n=== Ratios ===\")\n",
        "normal_ratios = compute_ratios(normal_video, numerator_cols=['likes', 'comments'], denominator_col='views')\n",
        "print(\"Normal video - sample ratios:\")\n",
        "print(normal_ratios.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Features\n",
        "\n",
        "Save the extracted features to disk for use in modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save features\n",
        "output_path = project_root / \"data\" / \"processed\" / \"temporal_features.parquet\"\n",
        "\n",
        "save_features(\n",
        "    features_df,\n",
        "    output_path=str(output_path),\n",
        "    output_format=\"parquet\",\n",
        ")\n",
        "\n",
        "print(f\"\\nFeatures saved successfully!\")\n",
        "print(f\"Total features: {len(feature_cols)}\")\n",
        "print(f\"Total videos: {len(features_df)}\")\n",
        "print(f\"Normal videos: {len(features_df[features_df['label'] == 'normal'])}\")\n",
        "print(f\"Fake videos: {len(features_df[features_df['label'] == 'fake'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Correlation Analysis\n",
        "\n",
        "Analyze correlations between features to identify redundancy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute correlation matrix for top features\n",
        "top_20_features = importance_df.head(20)['feature'].tolist()\n",
        "top_20_features = [f for f in top_20_features if f in features_df.columns]\n",
        "\n",
        "corr_matrix = features_df[top_20_features].corr()\n",
        "\n",
        "# plot correlation heatmap\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=False,\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    square=True,\n",
        "    fmt='.2f',\n",
        ")\n",
        "plt.title('Feature Correlation Matrix (Top 20 Features)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# find highly correlated feature pairs\n",
        "print(\"\\nHighly correlated feature pairs (|correlation| > 0.8):\")\n",
        "high_corr_pairs = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        corr_val = corr_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.8:\n",
        "            high_corr_pairs.append((\n",
        "                corr_matrix.columns[i],\n",
        "                corr_matrix.columns[j],\n",
        "                corr_val\n",
        "            ))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    for feat1, feat2, corr in high_corr_pairs[:10]:\n",
        "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
        "else:\n",
        "    print(\"No highly correlated pairs found (threshold: 0.8)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary\n",
        "\n",
        "Summary of feature engineering results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1. Total features extracted: {len(feature_cols)}\")\n",
        "print(f\"   - Rolling statistics: {len([f for f in feature_cols if 'rolling' in f])}\")\n",
        "print(f\"   - Burst features: {len([f for f in feature_cols if 'peaks' in f or 'max_mean' in f])}\")\n",
        "print(f\"   - Autocorrelation: {len([f for f in feature_cols if 'autocorr' in f])}\")\n",
        "print(f\"   - Entropy/Regularity: {len([f for f in feature_cols if 'entropy' in f or 'regularity' in f])}\")\n",
        "print(f\"   - Ratio features: {len([f for f in feature_cols if 'ratio' in f])}\")\n",
        "print(f\"   - Trend features: {len([f for f in feature_cols if 'trend' in f])}\")\n",
        "\n",
        "print(f\"\\n2. Statistical significance:\")\n",
        "significant_features = importance_df[importance_df['significant'] == True]\n",
        "print(f\"   - Significant features (p < 0.05): {len(significant_features)}\")\n",
        "print(f\"   - Top 5 most discriminative:\")\n",
        "for idx, row in importance_df.head(5).iterrows():\n",
        "    print(f\"     {row['feature']}: effect_size={row['effect_size']:.3f}, p={row['p_value']:.4f}\")\n",
        "\n",
        "print(f\"\\n3. Feature quality:\")\n",
        "print(f\"   - Features saved to: {output_path}\")\n",
        "print(f\"   - Ready for baseline models (tree-based, anomaly detection)\")\n",
        "print(f\"   - Ready for deep learning models (as additional features)\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
