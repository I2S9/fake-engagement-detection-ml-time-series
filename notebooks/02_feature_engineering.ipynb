{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Temporal Features\n",
    "\n",
    "This notebook extracts and analyzes temporal features from engagement time series data.\n",
    "\n",
    "Features include:\n",
    "- Rolling statistics (mean, std, min, max)\n",
    "- Ratio features (likes/views, comments/views)\n",
    "- Burst detection (peaks, max/mean ratio)\n",
    "- Autocorrelation at different lags\n",
    "- Entropy and regularity measures\n",
    "- Trend features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# create output directory for plots\n",
    "output_dir = project_root / \"outputs\" / \"figures\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set plotting style - FORCE DISPLAY\n",
    "plt.ion()  # interactive mode\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# try to import PCA/UMAP for visualization\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    HAS_PCA = True\n",
    "except ImportError:\n",
    "    HAS_PCA = False\n",
    "    print(\"Warning: sklearn not available for PCA visualization\")\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    HAS_UMAP = False\n",
    "    print(\"Warning: umap-learn not available for UMAP visualization\")\n",
    "\n",
    "# import project modules\n",
    "from src.data.load_data import load_data\n",
    "from src.features.temporal_features import (\n",
    "    extract_temporal_features,\n",
    "    save_features,\n",
    "    compute_rolling_statistics,\n",
    "    compute_ratios,\n",
    "    detect_bursts,\n",
    "    compute_autocorrelation,\n",
    "    compute_entropy,\n",
    "    compute_trend_features,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load the preprocessed time series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = project_root / \"data\" / \"raw\" / \"engagement.parquet\"\n",
    "df = load_data(data_path)\n",
    "\n",
    "# adapt column names if needed\n",
    "if 'user_id' in df.columns and 'id' not in df.columns:\n",
    "    df['id'] = df['user_id']\n",
    "if 'is_fake_series' in df.columns and 'label' not in df.columns:\n",
    "    df['label'] = df['is_fake_series'].map({True: 'fake', False: 'normal'})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of unique users: {df['id'].nunique()}\")\n",
    "if 'is_fake_series' in df.columns:\n",
    "    print(f\"\\nFake series distribution:\")\n",
    "    print(df['is_fake_series'].value_counts())\n",
    "if 'label' in df.columns:\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Temporal Features\n",
    "\n",
    "Extract all temporal features for each video ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features aggregated per user ID\n",
    "print(\"Extracting temporal features...\")\n",
    "features_df = extract_temporal_features(\n",
    "    df,\n",
    "    id_column=\"id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    window_sizes=[6, 12, 24],\n",
    "    autocorr_lags=[1, 6, 12, 24],\n",
    "    aggregate_per_id=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures extracted: {features_df.shape}\")\n",
    "print(f\"Number of feature columns: {len([c for c in features_df.columns if c not in ['id', 'label']])}\")\n",
    "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
    "print(f\"\\nFeature columns (first 20):\")\n",
    "print(feature_cols[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rolling Features Visualization\n",
    "\n",
    "Show rolling mean, variance, and autocorrelation features for sample series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a sample user series\n",
    "sample_user_id = df['id'].unique()[0]\n",
    "sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "\n",
    "# compute rolling features for this series\n",
    "rolling_mean_6 = sample_series['views'].rolling(window=6, min_periods=1).mean()\n",
    "rolling_std_6 = sample_series['views'].rolling(window=6, min_periods=1).std()\n",
    "rolling_mean_24 = sample_series['views'].rolling(window=24, min_periods=1).mean()\n",
    "rolling_std_24 = sample_series['views'].rolling(window=24, min_periods=1).std()\n",
    "\n",
    "# compute autocorrelation (simplified - using rolling correlation)\n",
    "autocorr_lag_1 = sample_series['views'].rolling(window=12, min_periods=2).apply(\n",
    "    lambda x: x.corr(x.shift(1)) if len(x.dropna()) > 1 else 0, raw=False\n",
    ")\n",
    "\n",
    "# plot series with rolling features (2-row panel)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# top panel: original series\n",
    "ax1 = axes[0]\n",
    "ax1.plot(sample_series['timestamp'], sample_series['views'], \n",
    "         label='Views', linewidth=2, color='blue', alpha=0.7)\n",
    "ax1.plot(sample_series['timestamp'], rolling_mean_6, \n",
    "         label='Rolling Mean (6h)', linewidth=1.5, color='green', linestyle='--')\n",
    "ax1.plot(sample_series['timestamp'], rolling_mean_24, \n",
    "         label='Rolling Mean (24h)', linewidth=1.5, color='orange', linestyle='--')\n",
    "ax1.fill_between(sample_series['timestamp'], \n",
    "                 rolling_mean_6 - rolling_std_6, \n",
    "                 rolling_mean_6 + rolling_std_6,\n",
    "                 alpha=0.2, color='green', label='Rolling Std (6h)')\n",
    "ax1.set_ylabel('Views', fontsize=12)\n",
    "ax1.set_title(f'Time Series with Rolling Statistics - User: {sample_user_id}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# bottom panel: feature scores\n",
    "ax2 = axes[1]\n",
    "ax2.plot(sample_series['timestamp'], rolling_std_6, \n",
    "         label='Rolling Std (6h)', linewidth=1.5, color='red')\n",
    "ax2.plot(sample_series['timestamp'], rolling_std_24, \n",
    "         label='Rolling Std (24h)', linewidth=1.5, color='purple')\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(sample_series['timestamp'], autocorr_lag_1, \n",
    "              label='Autocorr (lag=1)', linewidth=1.5, color='brown', linestyle=':')\n",
    "ax2_twin.set_ylabel('Autocorrelation', fontsize=12, color='brown')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='brown')\n",
    "ax2.set_xlabel('Timestamp', fontsize=12)\n",
    "ax2.set_ylabel('Rolling Std', fontsize=12)\n",
    "ax2.set_title('Feature Scores: Rolling Variance and Autocorrelation', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2_twin.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_rolling_features_panel.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA/UMAP Visualization - Class Separation\n",
    "\n",
    "Visualize feature space in 2D using PCA or UMAP to see class separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features first if not done\n",
    "if 'features_df' not in locals():\n",
    "    print(\"Extracting temporal features...\")\n",
    "    features_df = extract_temporal_features(\n",
    "        df,\n",
    "        id_column=\"id\",\n",
    "        timestamp_column=\"timestamp\",\n",
    "        window_sizes=[6, 12, 24],\n",
    "        autocorr_lags=[1, 6, 12, 24],\n",
    "        aggregate_per_id=True,\n",
    "    )\n",
    "    print(f\"Features extracted: {features_df.shape}\")\n",
    "\n",
    "# prepare data for PCA/UMAP\n",
    "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
    "X = features_df[feature_cols].fillna(0).values\n",
    "y = features_df['label'].map({'normal': 0, 'fake': 1}).values\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA visualization\n",
    "if HAS_PCA:\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # PCA plot\n",
    "    normal_mask = y == 0\n",
    "    fake_mask = y == 1\n",
    "    \n",
    "    axes[0].scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], \n",
    "                    alpha=0.6, label='Normal', color='blue', s=30)\n",
    "    axes[0].scatter(X_pca[fake_mask, 0], X_pca[fake_mask, 1], \n",
    "                    alpha=0.6, label='Fake', color='red', s=30)\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "    axes[0].set_title('PCA Visualization - Feature Space', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # UMAP visualization\n",
    "    if HAS_UMAP:\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "        X_umap = reducer.fit_transform(X_scaled)\n",
    "        \n",
    "        axes[1].scatter(X_umap[normal_mask, 0], X_umap[normal_mask, 1], \n",
    "                        alpha=0.6, label='Normal', color='blue', s=30)\n",
    "        axes[1].scatter(X_umap[fake_mask, 0], X_umap[fake_mask, 1], \n",
    "                        alpha=0.6, label='Fake', color='red', s=30)\n",
    "        axes[1].set_xlabel('UMAP 1', fontsize=12)\n",
    "        axes[1].set_ylabel('UMAP 2', fontsize=12)\n",
    "        axes[1].set_title('UMAP Visualization - Feature Space', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'UMAP not available', \n",
    "                     ha='center', va='center', fontsize=14)\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"02_pca_umap_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "else:\n",
    "    print(\"PCA not available. Install sklearn to enable this visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features aggregated per video ID\n",
    "print(\"Extracting temporal features...\")\n",
    "features_df = extract_temporal_features(\n",
    "    df,\n",
    "    id_column=\"id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    window_sizes=[6, 12, 24],\n",
    "    autocorr_lags=[1, 6, 12, 24],\n",
    "    aggregate_per_id=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures extracted: {features_df.shape}\")\n",
    "print(f\"Number of feature columns: {len([c for c in features_df.columns if c not in ['id', 'label']])}\")\n",
    "print(f\"\\nFeature columns (first 20):\")\n",
    "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
    "print(feature_cols[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Heatmap\n",
    "\n",
    "Visualize feature importance as a heatmap for easy identification of discriminative features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature importance heatmap\n",
    "if 'importance_df' in locals() and len(importance_df) > 0:\n",
    "    top_30_features = importance_df.head(30)\n",
    "    \n",
    "    # prepare data for heatmap\n",
    "    heatmap_data = top_30_features[['normal_mean', 'fake_mean', 'effect_size', 'p_value']].copy()\n",
    "    heatmap_data['-log10(p_value)'] = -np.log10(heatmap_data['p_value'] + 1e-10)\n",
    "    heatmap_data = heatmap_data[['normal_mean', 'fake_mean', 'effect_size', '-log10(p_value)']]\n",
    "    heatmap_data.index = top_30_features['feature']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, max(10, len(top_30_features) * 0.4)))\n",
    "    sns.heatmap(heatmap_data.T, annot=False, fmt='.2f', cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Value'}, ax=ax, linewidths=0.5)\n",
    "    ax.set_title('Top 30 Feature Importance Heatmap', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Feature', fontsize=12)\n",
    "    ax.set_ylabel('Metric', fontsize=12)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_01_plot.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # create effect size bar chart\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, max(8, len(top_30_features) * 0.3)))\n",
    "    colors = ['red' if p < 0.05 else 'gray' for p in top_30_features['p_value']]\n",
    "    ax.barh(range(len(top_30_features)), top_30_features['effect_size'], color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(top_30_features)))\n",
    "    ax.set_yticklabels(top_30_features['feature'], fontsize=9)\n",
    "    ax.set_xlabel('Effect Size', fontsize=12)\n",
    "    ax.set_title('Top 30 Features by Effect Size (Red = Significant, p<0.05)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_02_plot.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not computed yet. Run the statistical analysis cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Statistics\n",
    "\n",
    "Analyze the distribution and statistics of extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics\n",
    "print(\"Feature statistics by label:\\n\")\n",
    "feature_cols = [c for c in features_df.columns if c not in ['id', 'label']]\n",
    "\n",
    "# select a subset of key features for display\n",
    "key_features = [\n",
    "    col for col in feature_cols \n",
    "    if any(x in col for x in ['max_mean_ratio', 'n_peaks', 'entropy', 'regularity', 'autocorr_lag_1', 'ratio_likes_views'])\n",
    "]\n",
    "\n",
    "print(f\"Key features statistics:\")\n",
    "print(features_df.groupby('label')[key_features].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distributions - Normal vs Fake\n",
    "\n",
    "Compare feature distributions between normal and fake engagement patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select key features for visualization\n",
    "key_features_viz = [\n",
    "    'views_max_mean_ratio',\n",
    "    'views_n_peaks',\n",
    "    'views_entropy',\n",
    "    'views_regularity',\n",
    "    'views_autocorr_lag_1',\n",
    "    'ratio_likes_views',\n",
    "    'likes_max_mean_ratio',\n",
    "    'likes_n_peaks',\n",
    "]\n",
    "\n",
    "# filter to available features\n",
    "key_features_viz = [f for f in key_features_viz if f in features_df.columns]\n",
    "\n",
    "# plot distributions\n",
    "n_features = len(key_features_viz)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(key_features_viz):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "    fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "    \n",
    "    ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "    ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# hide extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_03_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# compute statistical significance for each feature\n",
    "feature_importance = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    normal_values = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "    fake_values = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "    \n",
    "    if len(normal_values) > 0 and len(fake_values) > 0:\n",
    "        # mann-whitney U test\n",
    "        try:\n",
    "            stat, p_value = mannwhitneyu(normal_values, fake_values, alternative='two-sided')\n",
    "            \n",
    "            # effect size (difference in means normalized by pooled std)\n",
    "            mean_diff = fake_values.mean() - normal_values.mean()\n",
    "            pooled_std = np.sqrt((normal_values.std()**2 + fake_values.std()**2) / 2)\n",
    "            effect_size = mean_diff / (pooled_std + 1e-6)\n",
    "            \n",
    "            feature_importance.append({\n",
    "                'feature': feature,\n",
    "                'normal_mean': normal_values.mean(),\n",
    "                'fake_mean': fake_values.mean(),\n",
    "                'mean_diff': mean_diff,\n",
    "                'effect_size': abs(effect_size),\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "importance_df = pd.DataFrame(feature_importance).sort_values('effect_size', ascending=False)\n",
    "\n",
    "print(\"Top 20 most discriminative features:\")\n",
    "print(importance_df.head(20)[['feature', 'normal_mean', 'fake_mean', 'effect_size', 'p_value', 'significant']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Distributions Analysis\n",
    "\n",
    "Visualize distributions of key features: variance, entropy, and burst features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select key features for distribution analysis\n",
    "variance_features = [f for f in feature_cols if 'rolling_std' in f or 'variance' in f][:4]\n",
    "entropy_features = [f for f in feature_cols if 'entropy' in f][:4]\n",
    "burst_features = [f for f in feature_cols if 'peaks' in f or 'max_mean' in f][:4]\n",
    "\n",
    "# create distribution plots\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "\n",
    "# variance features\n",
    "for idx, feature in enumerate(variance_features):\n",
    "    if idx < 4:\n",
    "        ax = axes[0, idx]\n",
    "        normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "        fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "        \n",
    "        ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "        ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
    "        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.set_title(f'Variance Feature: {feature.split(\"_\")[-1]}', fontsize=11, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# entropy features\n",
    "for idx, feature in enumerate(entropy_features):\n",
    "    if idx < 4:\n",
    "        ax = axes[1, idx]\n",
    "        normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "        fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "        \n",
    "        ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "        ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
    "        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.set_title(f'Entropy Feature: {feature.split(\"_\")[0]}', fontsize=11, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# burst features\n",
    "for idx, feature in enumerate(burst_features):\n",
    "    if idx < 4:\n",
    "        ax = axes[2, idx]\n",
    "        normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "        fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "        \n",
    "        ax.hist(normal_data, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "        ax.hist(fake_data, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
    "        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.set_title(f'Burst Feature: {feature.split(\"_\")[-1]}', fontsize=11, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions: Variance, Entropy, and Burst Features', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_04_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Before/After Normalization Visualization\n",
    "\n",
    "Compare feature distributions before and after normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features without normalization\n",
    "features_before = extract_temporal_features(\n",
    "    df,\n",
    "    id_column=\"id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    aggregate_per_id=True,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# extract features with normalization (standardize)\n",
    "from src.data.preprocess import normalize_engagement_metrics\n",
    "df_normalized = df.copy()\n",
    "df_normalized[['views', 'likes', 'comments', 'shares']] = normalize_engagement_metrics(\n",
    "    df_normalized[['views', 'likes', 'comments', 'shares']],\n",
    "    method='standardize'\n",
    ")\n",
    "\n",
    "features_after = extract_temporal_features(\n",
    "    df_normalized,\n",
    "    id_column=\"id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    aggregate_per_id=True,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# select a few key features for comparison\n",
    "key_features = ['views_rolling_mean_6', 'views_rolling_std_6', 'views_entropy', 'views_n_peaks']\n",
    "key_features = [f for f in key_features if f in features_before.columns and f in features_after.columns]\n",
    "\n",
    "# create before/after comparison plots\n",
    "fig, axes = plt.subplots(2, len(key_features), figsize=(5 * len(key_features), 10))\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    # before normalization\n",
    "    ax_before = axes[0, idx]\n",
    "    normal_before = features_before[features_before['label'] == 'normal'][feature].dropna()\n",
    "    fake_before = features_before[features_before['label'] == 'fake'][feature].dropna()\n",
    "    \n",
    "    ax_before.hist(normal_before, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "    ax_before.hist(fake_before, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
    "    ax_before.set_xlabel('Value', fontsize=10)\n",
    "    ax_before.set_ylabel('Density', fontsize=10)\n",
    "    ax_before.set_title(f'Before Normalization\\\\n{feature}', fontsize=11, fontweight='bold')\n",
    "    ax_before.legend()\n",
    "    ax_before.grid(True, alpha=0.3)\n",
    "    \n",
    "    # after normalization\n",
    "    ax_after = axes[1, idx]\n",
    "    normal_after = features_after[features_after['label'] == 'normal'][feature].dropna()\n",
    "    fake_after = features_after[features_after['label'] == 'fake'][feature].dropna()\n",
    "    \n",
    "    ax_after.hist(normal_after, bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "    ax_after.hist(fake_after, bins=30, alpha=0.6, label='Fake', color='red', density=True)\n",
    "    ax_after.set_xlabel('Value', fontsize=10)\n",
    "    ax_after.set_ylabel('Density', fontsize=10)\n",
    "    ax_after.set_title(f'After Normalization\\\\n{feature}', fontsize=11, fontweight='bold')\n",
    "    ax_after.legend()\n",
    "    ax_after.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions: Before vs After Normalization', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_05_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# print statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALIZATION IMPACT\")\n",
    "print(\"=\" * 60)\n",
    "for feature in key_features:\n",
    "    print(f\"\\\\n{feature}:\")\n",
    "    print(f\"  Before - Normal mean: {features_before[features_before['label'] == 'normal'][feature].mean():.4f}, std: {features_before[features_before['label'] == 'normal'][feature].std():.4f}\")\n",
    "    print(f\"  Before - Fake mean: {features_before[features_before['label'] == 'fake'][feature].mean():.4f}, std: {features_before[features_before['label'] == 'fake'][feature].std():.4f}\")\n",
    "    print(f\"  After - Normal mean: {features_after[features_after['label'] == 'normal'][feature].mean():.4f}, std: {features_after[features_after['label'] == 'normal'][feature].std():.4f}\")\n",
    "    print(f\"  After - Fake mean: {features_after[features_after['label'] == 'fake'][feature].mean():.4f}, std: {features_after[features_after['label'] == 'fake'][feature].std():.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 9 features\n",
    "top_features = importance_df.head(9)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    normal_data = features_df[features_df['label'] == 'normal'][feature].dropna()\n",
    "    fake_data = features_df[features_df['label'] == 'fake'][feature].dropna()\n",
    "    \n",
    "    # box plot\n",
    "    bp = ax.boxplot([normal_data, fake_data], labels=['Normal', 'Fake'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('blue')\n",
    "    bp['boxes'][0].set_alpha(0.6)\n",
    "    bp['boxes'][1].set_facecolor('red')\n",
    "    bp['boxes'][1].set_alpha(0.6)\n",
    "    \n",
    "    ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=10)\n",
    "    ax.set_title(f'Top Feature #{idx+1}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_06_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: Feature Extraction for Single Video\n",
    "\n",
    "Demonstrate feature extraction for a single video to understand the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one normal and one fake video\n",
    "normal_id = df[df['label'] == 'normal']['id'].iloc[0]\n",
    "fake_id = df[df['label'] == 'fake']['id'].iloc[0]\n",
    "\n",
    "normal_video = df[df['id'] == normal_id].sort_values('timestamp')\n",
    "fake_video = df[df['id'] == fake_id].sort_values('timestamp')\n",
    "\n",
    "print(f\"Normal video: {normal_id}\")\n",
    "print(f\"Fake video: {fake_id}\\n\")\n",
    "\n",
    "# compute features for views metric\n",
    "print(\"=== Rolling Statistics (views) ===\")\n",
    "normal_rolling = compute_rolling_statistics(normal_video['views'], window_sizes=[6, 12, 24])\n",
    "print(\"Normal video - sample rolling features:\")\n",
    "print(normal_rolling.head(10))\n",
    "\n",
    "print(\"\\n=== Burst Detection (views) ===\")\n",
    "normal_bursts = detect_bursts(normal_video['views'])\n",
    "fake_bursts = detect_bursts(fake_video['views'])\n",
    "print(f\"Normal: {normal_bursts}\")\n",
    "print(f\"Fake: {fake_bursts}\")\n",
    "\n",
    "print(\"\\n=== Autocorrelation (views) ===\")\n",
    "normal_autocorr = compute_autocorrelation(normal_video['views'], lags=[1, 6, 12, 24])\n",
    "fake_autocorr = compute_autocorrelation(fake_video['views'], lags=[1, 6, 12, 24])\n",
    "print(f\"Normal: {normal_autocorr}\")\n",
    "print(f\"Fake: {fake_autocorr}\")\n",
    "\n",
    "print(\"\\n=== Entropy (views) ===\")\n",
    "normal_entropy = compute_entropy(normal_video['views'])\n",
    "fake_entropy = compute_entropy(fake_video['views'])\n",
    "print(f\"Normal: {normal_entropy}\")\n",
    "print(f\"Fake: {fake_entropy}\")\n",
    "\n",
    "print(\"\\n=== Ratios ===\")\n",
    "normal_ratios = compute_ratios(normal_video, numerator_cols=['likes', 'comments'], denominator_col='views')\n",
    "print(\"Normal video - sample ratios:\")\n",
    "print(normal_ratios.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Features\n",
    "\n",
    "Save the extracted features to disk for use in modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "output_path = project_root / \"data\" / \"processed\" / \"temporal_features.parquet\"\n",
    "\n",
    "save_features(\n",
    "    features_df,\n",
    "    output_path=str(output_path),\n",
    "    output_format=\"parquet\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures saved successfully!\")\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Total videos: {len(features_df)}\")\n",
    "print(f\"Normal videos: {len(features_df[features_df['label'] == 'normal'])}\")\n",
    "print(f\"Fake videos: {len(features_df[features_df['label'] == 'fake'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Correlation Analysis\n",
    "\n",
    "Analyze correlations between features to identify redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrix for top features\n",
    "top_20_features = importance_df.head(20)['feature'].tolist()\n",
    "top_20_features = [f for f in top_20_features if f in features_df.columns]\n",
    "\n",
    "corr_matrix = features_df[top_20_features].corr()\n",
    "\n",
    "# plot correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=False,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    fmt='.2f',\n",
    ")\n",
    "plt.title('Feature Correlation Matrix (Top 20 Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"02_feature_engineering_07_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# find highly correlated feature pairs\n",
    "print(\"\\nHighly correlated feature pairs (|correlation| > 0.8):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in high_corr_pairs[:10]:\n",
    "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated pairs found (threshold: 0.8)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Summary of feature engineering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Total features extracted: {len(feature_cols)}\")\n",
    "print(f\"   - Rolling statistics: {len([f for f in feature_cols if 'rolling' in f])}\")\n",
    "print(f\"   - Burst features: {len([f for f in feature_cols if 'peaks' in f or 'max_mean' in f])}\")\n",
    "print(f\"   - Autocorrelation: {len([f for f in feature_cols if 'autocorr' in f])}\")\n",
    "print(f\"   - Entropy/Regularity: {len([f for f in feature_cols if 'entropy' in f or 'regularity' in f])}\")\n",
    "print(f\"   - Ratio features: {len([f for f in feature_cols if 'ratio' in f])}\")\n",
    "print(f\"   - Trend features: {len([f for f in feature_cols if 'trend' in f])}\")\n",
    "\n",
    "print(f\"\\n2. Statistical significance:\")\n",
    "significant_features = importance_df[importance_df['significant'] == True]\n",
    "print(f\"   - Significant features (p < 0.05): {len(significant_features)}\")\n",
    "print(f\"   - Top 5 most discriminative:\")\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"     {row['feature']}: effect_size={row['effect_size']:.3f}, p={row['p_value']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Feature quality:\")\n",
    "print(f\"   - Features saved to: {output_path}\")\n",
    "print(f\"   - Ready for baseline models (tree-based, anomaly detection)\")\n",
    "print(f\"   - Ready for deep learning models (as additional features)\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
