{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training, Evaluation and Comparison\n",
        "\n",
        "This notebook trains and compares baseline models (Logistic Regression, Random Forest, Isolation Forest, LOF) with sequential deep learning models (LSTM, TCN, Autoencoder) for fake engagement detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# set plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# import project modules\n",
        "from src.data.preprocess import load_and_preprocess\n",
        "from src.data.sequence_preparation import prepare_sequences_for_training\n",
        "from src.data.dataset import create_dataloaders_from_dict\n",
        "from src.features.temporal_features import extract_temporal_features\n",
        "from src.training.train import (\n",
        "    train_multiple_baselines,\n",
        "    train_model_from_config,\n",
        ")\n",
        "from src.training.evaluate import (\n",
        "    compare_models,\n",
        "    compare_all_models,\n",
        "    evaluate_sequential_model,\n",
        "    compute_metrics,\n",
        ")\n",
        "from src.utils.config import load_config, update_config_with_data\n",
        "\n",
        "# set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Configuration and Data\n",
        "\n",
        "Load configuration and prepare data for both baseline and sequential models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load configuration\n",
        "config = load_config()\n",
        "print(\"Configuration loaded successfully\")\n",
        "\n",
        "# load preprocessed time series data\n",
        "data_path = project_root / \"data\" / \"raw\" / \"engagement_timeseries.parquet\"\n",
        "df = load_and_preprocess(\n",
        "    file_path=str(data_path),\n",
        "    target_timezone=\"UTC\",\n",
        "    resample_frequency=\"h\",\n",
        "    handle_missing=True,\n",
        "    missing_method=\"forward\",\n",
        "    normalize=False,\n",
        ")\n",
        "\n",
        "print(f\"\\nTime series data shape: {df.shape}\")\n",
        "print(f\"Number of videos: {df['id'].nunique()}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train Baseline Models\n",
        "\n",
        "Train baseline models on temporal features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract temporal features\n",
        "print(\"Extracting temporal features...\")\n",
        "features_df = extract_temporal_features(\n",
        "    df,\n",
        "    id_column=\"id\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    window_sizes=[6, 12, 24],\n",
        "    autocorr_lags=[1, 6, 12, 24],\n",
        "    aggregate_per_id=True,\n",
        ")\n",
        "\n",
        "print(f\"Features extracted: {features_df.shape}\")\n",
        "\n",
        "# train baseline models\n",
        "baseline_model_types = ['logistic_regression', 'random_forest', 'isolation_forest', 'lof']\n",
        "baseline_results = train_multiple_baselines(\n",
        "    features_df,\n",
        "    model_types=baseline_model_types,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    save_dir=str(project_root / \"models\" / \"baselines\"),\n",
        ")\n",
        "\n",
        "print(f\"\\nBaseline models trained: {len(baseline_results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Sequences for Sequential Models\n",
        "\n",
        "Prepare time series sequences for LSTM, TCN, and Autoencoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare sequences\n",
        "data_config = config.get(\"data\", {})\n",
        "seq_len = data_config.get(\"seq_len\", 48)\n",
        "\n",
        "print(\"Preparing sequences for sequential models...\")\n",
        "sequence_data = prepare_sequences_for_training(\n",
        "    df,\n",
        "    seq_len=seq_len,\n",
        "    stride=data_config.get(\"stride\", 1),\n",
        "    normalize=data_config.get(\"normalize\", True),\n",
        "    normalization_method=data_config.get(\"normalization_method\", \"standardize\"),\n",
        "    normalize_per_series=data_config.get(\"normalize_per_series\", False),\n",
        "    test_size=data_config.get(\"test_size\", 0.2),\n",
        "    val_size=data_config.get(\"val_size\", 0.1),\n",
        "    random_state=config.get(\"training\", {}).get(\"random_seed\", 42),\n",
        ")\n",
        "\n",
        "print(f\"Sequences prepared:\")\n",
        "print(f\"  Train: {sequence_data['X_train'].shape}\")\n",
        "print(f\"  Val: {sequence_data['X_val'].shape}\")\n",
        "print(f\"  Test: {sequence_data['X_test'].shape}\")\n",
        "\n",
        "# update config with data dimensions\n",
        "input_size = len(sequence_data['feature_names'])\n",
        "config = update_config_with_data(config, input_size=input_size, seq_len=seq_len)\n",
        "\n",
        "# create dataloaders\n",
        "dataloaders = create_dataloaders_from_dict(\n",
        "    sequence_data,\n",
        "    batch_size=data_config.get(\"batch_size\", 32),\n",
        "    shuffle_train=True,\n",
        "    num_workers=data_config.get(\"num_workers\", 0),\n",
        "    pin_memory=data_config.get(\"pin_memory\", False),\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoaders created: {list(dataloaders.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Sequential Models\n",
        "\n",
        "Train LSTM, TCN, and Autoencoder models with early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# train sequential models\n",
        "sequential_models = ['lstm', 'tcn', 'autoencoder']\n",
        "sequential_results = {}\n",
        "training_histories = {}\n",
        "\n",
        "for model_type in sequential_models:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_type.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model, history = train_model_from_config(\n",
        "        model_type=model_type,\n",
        "        dataloaders=dataloaders,\n",
        "        config=config,\n",
        "        device=device,\n",
        "        save_dir=str(project_root / \"models\" / \"sequential\"),\n",
        "    )\n",
        "    \n",
        "    sequential_results[model_type] = (\n",
        "        model,\n",
        "        dataloaders['test'],\n",
        "        device,\n",
        "        model_type\n",
        "    )\n",
        "    training_histories[model_type] = history\n",
        "    \n",
        "    print(f\"{model_type.upper()} training completed\")\n",
        "\n",
        "print(f\"\\nSequential models trained: {len(sequential_results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Curves Visualization\n",
        "\n",
        "Visualize training and validation curves for sequential models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for idx, (model_type, history) in enumerate(training_histories.items()):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    \n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    ax.plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
        "    ax.plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    \n",
        "    if history['train_accuracy'] and any(history['train_accuracy']):\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(epochs, history['train_accuracy'], label='Train Acc', \n",
        "                linewidth=2, linestyle='--', color='green')\n",
        "        ax2.plot(epochs, history['val_accuracy'], label='Val Acc', \n",
        "                linewidth=2, linestyle='--', color='orange')\n",
        "        ax2.set_ylabel('Accuracy', fontsize=10)\n",
        "        ax2.legend(loc='upper right')\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Loss', fontsize=12)\n",
        "    ax.set_title(f'{model_type.upper()} Training Curves', fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# hide unused subplot\n",
        "if len(training_histories) < 4:\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Sequential Models\n",
        "\n",
        "Evaluate sequential models on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate sequential models\n",
        "sequential_metrics = {}\n",
        "\n",
        "for model_type, (model, test_loader, device, _) in sequential_results.items():\n",
        "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
        "    y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
        "    sequential_metrics[model_type] = metrics\n",
        "    \n",
        "    print(f\"  AUC: {metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
        "\n",
        "sequential_metrics_df = pd.DataFrame(sequential_metrics).T\n",
        "print(\"\\nSequential Models Metrics:\")\n",
        "print(sequential_metrics_df[['auc', 'precision', 'recall', 'f1', 'false_positive_rate']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. LSTM vs TCN Comparison\n",
        "\n",
        "Compare LSTM and TCN models performance and training characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare LSTM vs TCN\n",
        "if 'lstm' in sequential_results and 'tcn' in sequential_results:\n",
        "    # get training histories\n",
        "    lstm_history = training_histories.get('lstm', {})\n",
        "    tcn_history = training_histories.get('tcn', {})\n",
        "    \n",
        "    # plot comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # loss comparison\n",
        "    ax = axes[0, 0]\n",
        "    if lstm_history and 'train_loss' in lstm_history:\n",
        "        epochs_lstm = range(1, len(lstm_history['train_loss']) + 1)\n",
        "        ax.plot(epochs_lstm, lstm_history['train_loss'], label='LSTM Train', linewidth=2, color='blue')\n",
        "        ax.plot(epochs_lstm, lstm_history['val_loss'], label='LSTM Val', linewidth=2, color='blue', linestyle='--')\n",
        "    if tcn_history and 'train_loss' in tcn_history:\n",
        "        epochs_tcn = range(1, len(tcn_history['train_loss']) + 1)\n",
        "        ax.plot(epochs_tcn, tcn_history['train_loss'], label='TCN Train', linewidth=2, color='red')\n",
        "        ax.plot(epochs_tcn, tcn_history['val_loss'], label='TCN Val', linewidth=2, color='red', linestyle='--')\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Loss', fontsize=12)\n",
        "    ax.set_title('Loss Comparison: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # accuracy comparison\n",
        "    ax = axes[0, 1]\n",
        "    if lstm_history and 'train_accuracy' in lstm_history and any(lstm_history['train_accuracy']):\n",
        "        epochs_lstm = range(1, len(lstm_history['train_accuracy']) + 1)\n",
        "        ax.plot(epochs_lstm, lstm_history['train_accuracy'], label='LSTM Train', linewidth=2, color='blue')\n",
        "        ax.plot(epochs_lstm, lstm_history['val_accuracy'], label='LSTM Val', linewidth=2, color='blue', linestyle='--')\n",
        "    if tcn_history and 'train_accuracy' in tcn_history and any(tcn_history['train_accuracy']):\n",
        "        epochs_tcn = range(1, len(tcn_history['train_accuracy']) + 1)\n",
        "        ax.plot(epochs_tcn, tcn_history['train_accuracy'], label='TCN Train', linewidth=2, color='red')\n",
        "        ax.plot(epochs_tcn, tcn_history['val_accuracy'], label='TCN Val', linewidth=2, color='red', linestyle='--')\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax.set_title('Accuracy Comparison: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # evaluate and compare metrics\n",
        "    lstm_model, lstm_loader, lstm_device, _ = sequential_results['lstm']\n",
        "    tcn_model, tcn_loader, tcn_device, _ = sequential_results['tcn']\n",
        "    \n",
        "    lstm_y_true, lstm_y_pred, lstm_y_proba = evaluate_sequential_model(lstm_model, lstm_loader, lstm_device, 'lstm')\n",
        "    tcn_y_true, tcn_y_pred, tcn_y_proba = evaluate_sequential_model(tcn_model, tcn_loader, tcn_device, 'tcn')\n",
        "    \n",
        "    lstm_metrics = compute_metrics(lstm_y_true, lstm_y_pred, lstm_y_proba)\n",
        "    tcn_metrics = compute_metrics(tcn_y_true, tcn_y_pred, tcn_y_proba)\n",
        "    \n",
        "    # metrics bar chart\n",
        "    ax = axes[1, 0]\n",
        "    metrics_names = ['AUC', 'Precision', 'Recall', 'F1']\n",
        "    lstm_values = [lstm_metrics['auc'], lstm_metrics['precision'], lstm_metrics['recall'], lstm_metrics['f1']]\n",
        "    tcn_values = [tcn_metrics['auc'], tcn_metrics['precision'], tcn_metrics['recall'], tcn_metrics['f1']]\n",
        "    \n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "    ax.bar(x - width/2, lstm_values, width, label='LSTM', color='blue', alpha=0.7)\n",
        "    ax.bar(x + width/2, tcn_values, width, label='TCN', color='red', alpha=0.7)\n",
        "    ax.set_xlabel('Metric', fontsize=12)\n",
        "    ax.set_ylabel('Score', fontsize=12)\n",
        "    ax.set_title('Performance Metrics: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics_names)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # ROC curves comparison\n",
        "    ax = axes[1, 1]\n",
        "    plot_roc_curve(lstm_y_true, lstm_y_proba, model_name='LSTM', ax=ax)\n",
        "    plot_roc_curve(tcn_y_true, tcn_y_proba, model_name='TCN', ax=ax)\n",
        "    ax.set_title('ROC Curves: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # print comparison\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LSTM vs TCN Comparison\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nLSTM Metrics:\")\n",
        "    print(f\"  AUC: {lstm_metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {lstm_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {lstm_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {lstm_metrics['f1']:.4f}\")\n",
        "    print(f\"\\nTCN Metrics:\")\n",
        "    print(f\"  AUC: {tcn_metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {tcn_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {tcn_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {tcn_metrics['f1']:.4f}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"Both LSTM and TCN models are required for comparison. Train them first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Sequential Models\n",
        "\n",
        "Evaluate sequential models on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate sequential models\n",
        "sequential_metrics = {}\n",
        "\n",
        "for model_type, (model, test_loader, device, _) in sequential_results.items():\n",
        "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
        "    y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
        "    sequential_metrics[model_type] = metrics\n",
        "    \n",
        "    print(f\"  AUC: {metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
        "\n",
        "sequential_metrics_df = pd.DataFrame(sequential_metrics).T\n",
        "print(\"\\nSequential Models Metrics:\")\n",
        "print(sequential_metrics_df[['auc', 'precision', 'recall', 'f1', 'false_positive_rate']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Baseline Models\n",
        "\n",
        "Compare baseline models performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare baseline models\n",
        "baseline_metrics_df = compare_models(baseline_results, plot=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comprehensive Model Comparison\n",
        "\n",
        "Compare all models (baselines vs sequential) to identify the best model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# comprehensive comparison\n",
        "all_metrics_df = compare_all_models(baseline_results, sequential_results, plot=True)\n",
        "\n",
        "# save results\n",
        "results_path = project_root / \"models\" / \"model_comparison_results.csv\"\n",
        "all_metrics_df.to_csv(results_path)\n",
        "print(f\"\\nResults saved to: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Error Analysis\n",
        "\n",
        "Analyze misclassifications and identify patterns in errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get best model predictions for error analysis\n",
        "best_model_name = all_metrics_df['auc'].idxmax()\n",
        "print(f\"Analyzing errors for best model: {best_model_name}\")\n",
        "\n",
        "if best_model_name in sequential_results:\n",
        "    # sequential model\n",
        "    model, test_loader, device, model_type = sequential_results[best_model_name]\n",
        "    y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "else:\n",
        "    # baseline model\n",
        "    model, X_test, y_test, y_pred, y_proba = baseline_results[best_model_name]\n",
        "    y_true = y_test\n",
        "\n",
        "# confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# confusion matrix heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Normal', 'Fake'], yticklabels=['Normal', 'Fake'])\n",
        "axes[0].set_xlabel('Predicted', fontsize=12)\n",
        "axes[0].set_ylabel('True', fontsize=12)\n",
        "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "\n",
        "# error distribution\n",
        "errors = y_true != y_pred\n",
        "if y_proba.ndim > 1:\n",
        "    y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
        "else:\n",
        "    y_proba_positive = y_proba\n",
        "\n",
        "axes[1].hist(y_proba_positive[errors & (y_true == 0)], bins=20, \n",
        "            alpha=0.6, label='False Positives', color='red', density=True)\n",
        "axes[1].hist(y_proba_positive[errors & (y_true == 1)], bins=20, \n",
        "            alpha=0.6, label='False Negatives', color='orange', density=True)\n",
        "axes[1].set_xlabel('Prediction Score', fontsize=12)\n",
        "axes[1].set_ylabel('Density', fontsize=12)\n",
        "axes[1].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print error statistics\n",
        "print(f\"\\nError Statistics for {best_model_name}:\")\n",
        "print(f\"  Total errors: {errors.sum()}\")\n",
        "print(f\"  False Positives: {(errors & (y_true == 0)).sum()}\")\n",
        "print(f\"  False Negatives: {(errors & (y_true == 1)).sum()}\")\n",
        "print(f\"  Error rate: {errors.mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"MODEL SELECTION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# best model\n",
        "best_model_name = all_metrics_df['auc'].idxmax()\n",
        "best_metrics = all_metrics_df.loc[best_model_name]\n",
        "\n",
        "print(f\"\\nChampion Model: {best_model_name}\")\n",
        "print(f\"  Model Type: {best_metrics['model_type']}\")\n",
        "print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
        "print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
        "print(f\"  F1-Score: {best_metrics['f1']:.4f}\")\n",
        "print(f\"  False Positive Rate: {best_metrics['false_positive_rate']:.4f}\")\n",
        "\n",
        "# comparison baseline vs sequential\n",
        "baseline_avg_auc = all_metrics_df[all_metrics_df['model_type'] == 'baseline']['auc'].mean()\n",
        "sequential_avg_auc = all_metrics_df[all_metrics_df['model_type'] == 'sequential']['auc'].mean()\n",
        "\n",
        "print(f\"\\nAverage Performance:\")\n",
        "print(f\"  Baseline models: {baseline_avg_auc:.4f}\")\n",
        "print(f\"  Sequential models: {sequential_avg_auc:.4f}\")\n",
        "print(f\"  Improvement: {(sequential_avg_auc - baseline_avg_auc):.4f} ({(sequential_avg_auc - baseline_avg_auc) / baseline_avg_auc * 100:.1f}%)\")\n",
        "\n",
        "# top 3 models\n",
        "print(f\"\\nTop 3 Models (by AUC):\")\n",
        "top_3 = all_metrics_df.nlargest(3, 'auc')\n",
        "for idx, (model_name, row) in enumerate(top_3.iterrows(), 1):\n",
        "    print(f\"  {idx}. {model_name}: AUC={row['auc']:.4f}, F1={row['f1']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
