{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training, Evaluation and Comparison\n",
        "\n",
        "This notebook trains and compares baseline models (Logistic Regression, Random Forest, Isolation Forest, LOF) with sequential deep learning models (LSTM, TCN, Autoencoder) for fake engagement detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# set plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# import project modules\n",
        "from src.data.preprocess import load_and_preprocess\n",
        "from src.data.sequence_preparation import prepare_sequences_for_training\n",
        "from src.data.dataset import create_dataloaders_from_dict\n",
        "from src.features.temporal_features import extract_temporal_features\n",
        "from src.training.train import (\n",
        "    train_multiple_baselines,\n",
        "    train_model_from_config,\n",
        ")\n",
        "from src.training.evaluate import (\n",
        "    compare_models,\n",
        "    compare_all_models,\n",
        "    evaluate_sequential_model,\n",
        "    compute_metrics,\n",
        ")\n",
        "from src.utils.config import load_config, update_config_with_data\n",
        "\n",
        "# set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Configuration and Data\n",
        "\n",
        "Load configuration and prepare data for both baseline and sequential models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load configuration\n",
        "config = load_config()\n",
        "print(\"Configuration loaded successfully\")\n",
        "\n",
        "# load preprocessed time series data\n",
        "data_path = project_root / \"data\" / \"raw\" / \"engagement_timeseries.parquet\"\n",
        "df = load_and_preprocess(\n",
        "    file_path=str(data_path),\n",
        "    target_timezone=\"UTC\",\n",
        "    resample_frequency=\"h\",\n",
        "    handle_missing=True,\n",
        "    missing_method=\"forward\",\n",
        "    normalize=False,\n",
        ")\n",
        "\n",
        "print(f\"\\nTime series data shape: {df.shape}\")\n",
        "print(f\"Number of videos: {df['id'].nunique()}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training Data Overview\n",
        "\n",
        "Visualize the training data distribution and characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize training data distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# class distribution\n",
        "ax = axes[0, 0]\n",
        "label_counts = features_df['label'].value_counts()\n",
        "colors = ['blue', 'red']\n",
        "bars = ax.bar(label_counts.index, label_counts.values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_xlabel('Label', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_title('Class Distribution in Training Data', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "for i, (label, count) in enumerate(label_counts.items()):\n",
        "    ax.text(i, count, str(count), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "# feature distribution heatmap (sample)\n",
        "ax = axes[0, 1]\n",
        "sample_features = feature_cols[:20] if len(feature_cols) > 20 else feature_cols\n",
        "sample_data = features_df[sample_features + ['label']].groupby('label')[sample_features].mean().T\n",
        "sns.heatmap(sample_data, annot=False, fmt='.1f', cmap='viridis', ax=ax, cbar_kws={'label': 'Mean Value'})\n",
        "ax.set_title('Feature Mean Values by Label (Sample)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Label', fontsize=12)\n",
        "ax.set_ylabel('Feature', fontsize=10)\n",
        "\n",
        "# sequence length distribution\n",
        "ax = axes[1, 0]\n",
        "if 'sequences_dict' in locals() and sequences_dict:\n",
        "    seq_lengths = [len(seq) for seq in sequences_dict.get('X', [])]\n",
        "    ax.hist(seq_lengths, bins=20, color='green', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax.set_xlabel('Sequence Length', fontsize=12)\n",
        "    ax.set_ylabel('Frequency', fontsize=12)\n",
        "    ax.set_title('Sequence Length Distribution', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'Sequences not prepared yet', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
        "    ax.set_title('Sequence Length Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# label distribution in sequences\n",
        "ax = axes[1, 1]\n",
        "if 'sequences_dict' in locals() and sequences_dict and 'y' in sequences_dict:\n",
        "    label_counts_seq = pd.Series(sequences_dict['y']).value_counts()\n",
        "    colors_seq = ['blue', 'red']\n",
        "    bars = ax.bar(['Normal', 'Fake'], [label_counts_seq.get(0, 0), label_counts_seq.get(1, 0)], \n",
        "                  color=colors_seq, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax.set_xlabel('Label', fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=12)\n",
        "    ax.set_title('Class Distribution in Sequences', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    for i, count in enumerate([label_counts_seq.get(0, 0), label_counts_seq.get(1, 0)]):\n",
        "        ax.text(i, count, str(count), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'Sequences not prepared yet', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
        "    ax.set_title('Class Distribution in Sequences', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train Baseline Models\n",
        "\n",
        "Train baseline models on temporal features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract temporal features\n",
        "print(\"Extracting temporal features...\")\n",
        "features_df = extract_temporal_features(\n",
        "    df,\n",
        "    id_column=\"id\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    window_sizes=[6, 12, 24],\n",
        "    autocorr_lags=[1, 6, 12, 24],\n",
        "    aggregate_per_id=True,\n",
        ")\n",
        "\n",
        "print(f\"Features extracted: {features_df.shape}\")\n",
        "\n",
        "# train baseline models\n",
        "baseline_model_types = ['logistic_regression', 'random_forest', 'isolation_forest', 'lof']\n",
        "baseline_results = train_multiple_baselines(\n",
        "    features_df,\n",
        "    model_types=baseline_model_types,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    save_dir=str(project_root / \"models\" / \"baselines\"),\n",
        ")\n",
        "\n",
        "print(f\"\\nBaseline models trained: {len(baseline_results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Sequences for Sequential Models\n",
        "\n",
        "Prepare time series sequences for LSTM, TCN, and Autoencoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare sequences\n",
        "data_config = config.get(\"data\", {})\n",
        "seq_len = data_config.get(\"seq_len\", 48)\n",
        "\n",
        "print(\"Preparing sequences for sequential models...\")\n",
        "sequence_data = prepare_sequences_for_training(\n",
        "    df,\n",
        "    seq_len=seq_len,\n",
        "    stride=data_config.get(\"stride\", 1),\n",
        "    normalize=data_config.get(\"normalize\", True),\n",
        "    normalization_method=data_config.get(\"normalization_method\", \"standardize\"),\n",
        "    normalize_per_series=data_config.get(\"normalize_per_series\", False),\n",
        "    test_size=data_config.get(\"test_size\", 0.2),\n",
        "    val_size=data_config.get(\"val_size\", 0.1),\n",
        "    random_state=config.get(\"training\", {}).get(\"random_seed\", 42),\n",
        ")\n",
        "\n",
        "print(f\"Sequences prepared:\")\n",
        "print(f\"  Train: {sequence_data['X_train'].shape}\")\n",
        "print(f\"  Val: {sequence_data['X_val'].shape}\")\n",
        "print(f\"  Test: {sequence_data['X_test'].shape}\")\n",
        "\n",
        "# update config with data dimensions\n",
        "input_size = len(sequence_data['feature_names'])\n",
        "config = update_config_with_data(config, input_size=input_size, seq_len=seq_len)\n",
        "\n",
        "# create dataloaders\n",
        "dataloaders = create_dataloaders_from_dict(\n",
        "    sequence_data,\n",
        "    batch_size=data_config.get(\"batch_size\", 32),\n",
        "    shuffle_train=True,\n",
        "    num_workers=data_config.get(\"num_workers\", 0),\n",
        "    pin_memory=data_config.get(\"pin_memory\", False),\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoaders created: {list(dataloaders.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Sequential Models\n",
        "\n",
        "Train LSTM, TCN, and Autoencoder models with early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# train sequential models\n",
        "sequential_models = ['lstm', 'tcn', 'autoencoder']\n",
        "sequential_results = {}\n",
        "training_histories = {}\n",
        "\n",
        "for model_type in sequential_models:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_type.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model, history = train_model_from_config(\n",
        "        model_type=model_type,\n",
        "        dataloaders=dataloaders,\n",
        "        config=config,\n",
        "        device=device,\n",
        "        save_dir=str(project_root / \"models\" / \"sequential\"),\n",
        "    )\n",
        "    \n",
        "    sequential_results[model_type] = (\n",
        "        model,\n",
        "        dataloaders['test'],\n",
        "        device,\n",
        "        model_type\n",
        "    )\n",
        "    training_histories[model_type] = history\n",
        "    \n",
        "    print(f\"{model_type.upper()} training completed\")\n",
        "\n",
        "print(f\"\\nSequential models trained: {len(sequential_results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Curves Visualization\n",
        "\n",
        "Visualize training and validation curves for sequential models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for idx, (model_type, history) in enumerate(training_histories.items()):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    \n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    ax.plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
        "    ax.plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    \n",
        "    if history['train_accuracy'] and any(history['train_accuracy']):\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(epochs, history['train_accuracy'], label='Train Acc', \n",
        "                linewidth=2, linestyle='--', color='green')\n",
        "        ax2.plot(epochs, history['val_accuracy'], label='Val Acc', \n",
        "                linewidth=2, linestyle='--', color='orange')\n",
        "        ax2.set_ylabel('Accuracy', fontsize=10)\n",
        "        ax2.legend(loc='upper right')\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Loss', fontsize=12)\n",
        "    ax.set_title(f'{model_type.upper()} Training Curves', fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# hide unused subplot\n",
        "if len(training_histories) < 4:\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Sequential Models\n",
        "\n",
        "Evaluate sequential models on test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1. Autoencoder Reconstruction Visualization\n",
        "\n",
        "Visualize original vs reconstructed series for the autoencoder model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize autoencoder reconstructions\n",
        "if 'autoencoder' in sequential_results:\n",
        "    ae_model, ae_loader, ae_device, _ = sequential_results['autoencoder']\n",
        "    ae_model.eval()\n",
        "    \n",
        "    # get a few sample sequences\n",
        "    sample_batch = next(iter(ae_loader))\n",
        "    X_sample, y_sample, _ = sample_batch\n",
        "    X_sample = X_sample.to(ae_device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        reconstructed = ae_model(X_sample)\n",
        "    \n",
        "    # convert to numpy\n",
        "    original_np = X_sample.cpu().numpy()\n",
        "    reconstructed_np = reconstructed.cpu().numpy()\n",
        "    \n",
        "    # plot a few examples\n",
        "    n_examples = min(4, len(X_sample))\n",
        "    fig, axes = plt.subplots(n_examples, 1, figsize=(16, 4 * n_examples))\n",
        "    if n_examples == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx in range(n_examples):\n",
        "        original_seq = original_np[idx, :, 0]  # first feature (views)\n",
        "        reconstructed_seq = reconstructed_np[idx, :, 0]\n",
        "        is_fake = y_sample[idx].item()\n",
        "        \n",
        "        # compute reconstruction error\n",
        "        error = np.abs(original_seq - reconstructed_seq)\n",
        "        anomaly_mask = error > np.percentile(error, 90)  # top 10% errors\n",
        "        \n",
        "        fig_plot, ax_plot = plot_reconstruction(\n",
        "            original_seq,\n",
        "            reconstructed_seq,\n",
        "            anomaly_mask,\n",
        "            title=f\"Autoencoder Reconstruction - Sample {idx+1} (Label: {'Fake' if is_fake else 'Normal'})\"\n",
        "        )\n",
        "        axes[idx].remove()\n",
        "        axes[idx] = ax_plot\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_dir / \"03_ae_reconstruction.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # print reconstruction statistics\n",
        "    mse = np.mean((original_np - reconstructed_np) ** 2)\n",
        "    print(f\"\\nAutoencoder Reconstruction Statistics:\")\n",
        "    print(f\"  Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"  Mean Absolute Error: {np.mean(np.abs(original_np - reconstructed_np)):.4f}\")\n",
        "else:\n",
        "    print(\"Autoencoder model not available. Train it first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate sequential models\n",
        "sequential_metrics = {}\n",
        "\n",
        "for model_type, (model, test_loader, device, _) in sequential_results.items():\n",
        "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
        "    y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
        "    sequential_metrics[model_type] = metrics\n",
        "    \n",
        "    print(f\"  AUC: {metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
        "\n",
        "sequential_metrics_df = pd.DataFrame(sequential_metrics).T\n",
        "print(\"\\nSequential Models Metrics:\")\n",
        "print(sequential_metrics_df[['auc', 'precision', 'recall', 'f1', 'false_positive_rate']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. LSTM vs TCN Comparison\n",
        "\n",
        "Compare LSTM and TCN models performance and training characteristics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Performance Heatmap\n",
        "\n",
        "Create a comprehensive heatmap comparing all baseline models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create baseline models performance heatmap\n",
        "if len(baseline_results) > 0:\n",
        "    baseline_metrics_dict = {}\n",
        "    for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
        "        metrics = compute_metrics(y_test, y_pred, y_proba)\n",
        "        baseline_metrics_dict[model_name] = metrics\n",
        "    \n",
        "    baseline_metrics_df = pd.DataFrame(baseline_metrics_dict).T\n",
        "    metrics_to_plot = ['auc', 'precision', 'recall', 'f1', 'false_positive_rate']\n",
        "    metrics_to_plot = [m for m in metrics_to_plot if m in baseline_metrics_df.columns]\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(max(8, len(baseline_results) * 1.5), 6))\n",
        "    sns.heatmap(baseline_metrics_df[metrics_to_plot].T, annot=True, fmt='.3f', \n",
        "                cmap='YlOrRd', cbar_kws={'label': 'Score'}, ax=ax, linewidths=0.5)\n",
        "    ax.set_title('Baseline Models Performance Heatmap', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Model', fontsize=12)\n",
        "    ax.set_ylabel('Metric', fontsize=12)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No baseline models loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare LSTM vs TCN\n",
        "if 'lstm' in sequential_results and 'tcn' in sequential_results:\n",
        "    # get training histories\n",
        "    lstm_history = training_histories.get('lstm', {})\n",
        "    tcn_history = training_histories.get('tcn', {})\n",
        "    \n",
        "    # plot comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # loss comparison\n",
        "    ax = axes[0, 0]\n",
        "    if lstm_history and 'train_loss' in lstm_history:\n",
        "        epochs_lstm = range(1, len(lstm_history['train_loss']) + 1)\n",
        "        ax.plot(epochs_lstm, lstm_history['train_loss'], label='LSTM Train', linewidth=2, color='blue')\n",
        "        ax.plot(epochs_lstm, lstm_history['val_loss'], label='LSTM Val', linewidth=2, color='blue', linestyle='--')\n",
        "    if tcn_history and 'train_loss' in tcn_history:\n",
        "        epochs_tcn = range(1, len(tcn_history['train_loss']) + 1)\n",
        "        ax.plot(epochs_tcn, tcn_history['train_loss'], label='TCN Train', linewidth=2, color='red')\n",
        "        ax.plot(epochs_tcn, tcn_history['val_loss'], label='TCN Val', linewidth=2, color='red', linestyle='--')\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Loss', fontsize=12)\n",
        "    ax.set_title('Loss Comparison: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # accuracy comparison\n",
        "    ax = axes[0, 1]\n",
        "    if lstm_history and 'train_accuracy' in lstm_history and any(lstm_history['train_accuracy']):\n",
        "        epochs_lstm = range(1, len(lstm_history['train_accuracy']) + 1)\n",
        "        ax.plot(epochs_lstm, lstm_history['train_accuracy'], label='LSTM Train', linewidth=2, color='blue')\n",
        "        ax.plot(epochs_lstm, lstm_history['val_accuracy'], label='LSTM Val', linewidth=2, color='blue', linestyle='--')\n",
        "    if tcn_history and 'train_accuracy' in tcn_history and any(tcn_history['train_accuracy']):\n",
        "        epochs_tcn = range(1, len(tcn_history['train_accuracy']) + 1)\n",
        "        ax.plot(epochs_tcn, tcn_history['train_accuracy'], label='TCN Train', linewidth=2, color='red')\n",
        "        ax.plot(epochs_tcn, tcn_history['val_accuracy'], label='TCN Val', linewidth=2, color='red', linestyle='--')\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax.set_title('Accuracy Comparison: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # evaluate and compare metrics\n",
        "    lstm_model, lstm_loader, lstm_device, _ = sequential_results['lstm']\n",
        "    tcn_model, tcn_loader, tcn_device, _ = sequential_results['tcn']\n",
        "    \n",
        "    lstm_y_true, lstm_y_pred, lstm_y_proba = evaluate_sequential_model(lstm_model, lstm_loader, lstm_device, 'lstm')\n",
        "    tcn_y_true, tcn_y_pred, tcn_y_proba = evaluate_sequential_model(tcn_model, tcn_loader, tcn_device, 'tcn')\n",
        "    \n",
        "    lstm_metrics = compute_metrics(lstm_y_true, lstm_y_pred, lstm_y_proba)\n",
        "    tcn_metrics = compute_metrics(tcn_y_true, tcn_y_pred, tcn_y_proba)\n",
        "    \n",
        "    # metrics bar chart\n",
        "    ax = axes[1, 0]\n",
        "    metrics_names = ['AUC', 'Precision', 'Recall', 'F1']\n",
        "    lstm_values = [lstm_metrics['auc'], lstm_metrics['precision'], lstm_metrics['recall'], lstm_metrics['f1']]\n",
        "    tcn_values = [tcn_metrics['auc'], tcn_metrics['precision'], tcn_metrics['recall'], tcn_metrics['f1']]\n",
        "    \n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "    ax.bar(x - width/2, lstm_values, width, label='LSTM', color='blue', alpha=0.7)\n",
        "    ax.bar(x + width/2, tcn_values, width, label='TCN', color='red', alpha=0.7)\n",
        "    ax.set_xlabel('Metric', fontsize=12)\n",
        "    ax.set_ylabel('Score', fontsize=12)\n",
        "    ax.set_title('Performance Metrics: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics_names)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # ROC curves comparison\n",
        "    ax = axes[1, 1]\n",
        "    plot_roc_curve(lstm_y_true, lstm_y_proba, model_name='LSTM', ax=ax)\n",
        "    plot_roc_curve(tcn_y_true, tcn_y_proba, model_name='TCN', ax=ax)\n",
        "    ax.set_title('ROC Curves: LSTM vs TCN', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # print comparison\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LSTM vs TCN Comparison\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nLSTM Metrics:\")\n",
        "    print(f\"  AUC: {lstm_metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {lstm_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {lstm_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {lstm_metrics['f1']:.4f}\")\n",
        "    print(f\"\\nTCN Metrics:\")\n",
        "    print(f\"  AUC: {tcn_metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {tcn_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {tcn_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {tcn_metrics['f1']:.4f}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"Both LSTM and TCN models are required for comparison. Train them first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Sequential Models\n",
        "\n",
        "Evaluate sequential models on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate sequential models\n",
        "sequential_metrics = {}\n",
        "\n",
        "for model_type, (model, test_loader, device, _) in sequential_results.items():\n",
        "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
        "    y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
        "    sequential_metrics[model_type] = metrics\n",
        "    \n",
        "    print(f\"  AUC: {metrics['auc']:.4f}\")\n",
        "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
        "\n",
        "sequential_metrics_df = pd.DataFrame(sequential_metrics).T\n",
        "print(\"\\nSequential Models Metrics:\")\n",
        "print(sequential_metrics_df[['auc', 'precision', 'recall', 'f1', 'false_positive_rate']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Baseline Models\n",
        "\n",
        "Compare baseline models performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare baseline models\n",
        "baseline_metrics_df = compare_models(baseline_results, plot=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comprehensive Model Comparison\n",
        "\n",
        "Compare all models (baselines vs sequential) to identify the best model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.1. Score Comparison by Attack Type\n",
        "\n",
        "Compare model scores across different attack types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compare scores by attack type\n",
        "if 'attack_type' in df.columns:\n",
        "    # get predictions from best model\n",
        "    if 'best_model_name' in locals() and best_model_name in sequential_results:\n",
        "        model, test_loader, device, model_type = sequential_results[best_model_name]\n",
        "        y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "        \n",
        "        # get attack types for test set (need to map back from sequences)\n",
        "        # for now, use a simplified approach: get attack types from fake series\n",
        "        fake_df = df[df.get('is_fake_series', df.get('label') == 'fake')]\n",
        "        attack_types = fake_df['attack_type'].unique()\n",
        "        \n",
        "        if len(attack_types) > 0:\n",
        "            # create box plots by attack type\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "            \n",
        "            # get scores for fake samples only\n",
        "            fake_mask = y_true == 1\n",
        "            fake_scores = y_proba[fake_mask] if y_proba.ndim == 1 else y_proba[fake_mask, 1] if y_proba.shape[1] > 1 else y_proba[fake_mask].flatten()\n",
        "            \n",
        "            # sample attack types (simplified - in real scenario would map from sequences)\n",
        "            # for visualization, we'll use the attack types from the dataset\n",
        "            n_fake = len(fake_scores)\n",
        "            n_attack_types = len(attack_types)\n",
        "            samples_per_type = n_fake // n_attack_types\n",
        "            \n",
        "            attack_type_scores = {}\n",
        "            for idx, attack_type in enumerate(attack_types):\n",
        "                start_idx = idx * samples_per_type\n",
        "                end_idx = start_idx + samples_per_type if idx < n_attack_types - 1 else n_fake\n",
        "                attack_type_scores[attack_type] = fake_scores[start_idx:end_idx]\n",
        "            \n",
        "            # box plot\n",
        "            axes[0].boxplot([attack_type_scores[at] for at in attack_types], labels=attack_types)\n",
        "            axes[0].set_ylabel('Prediction Score', fontsize=12)\n",
        "            axes[0].set_title('Score Distribution by Attack Type', fontsize=14, fontweight='bold')\n",
        "            axes[0].tick_params(axis='x', rotation=45)\n",
        "            axes[0].grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            # bar chart of mean scores\n",
        "            mean_scores = [np.mean(attack_type_scores[at]) for at in attack_types]\n",
        "            axes[1].bar(range(len(attack_types)), mean_scores, color='red', alpha=0.7)\n",
        "            axes[1].set_xticks(range(len(attack_types)))\n",
        "            axes[1].set_xticklabels(attack_types, rotation=45, ha='right')\n",
        "            axes[1].set_ylabel('Mean Prediction Score', fontsize=12)\n",
        "            axes[1].set_title('Mean Score by Attack Type', fontsize=14, fontweight='bold')\n",
        "            axes[1].grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_dir / \"03_scores_by_attack_type.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            # print statistics\n",
        "            print(\"\\nScore Statistics by Attack Type:\")\n",
        "            for attack_type in attack_types:\n",
        "                scores = attack_type_scores[attack_type]\n",
        "                print(f\"  {attack_type}: mean={np.mean(scores):.4f}, std={np.std(scores):.4f}\")\n",
        "        else:\n",
        "            print(\"No attack types found in dataset.\")\n",
        "    else:\n",
        "        print(\"Best model not available. Train models first.\")\n",
        "else:\n",
        "    print(\"Attack type column not found in dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# comprehensive comparison\n",
        "all_metrics_df = compare_all_models(baseline_results, sequential_results, plot=True)\n",
        "\n",
        "# save results\n",
        "results_path = project_root / \"models\" / \"model_comparison_results.csv\"\n",
        "all_metrics_df.to_csv(results_path)\n",
        "print(f\"\\nResults saved to: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Error Analysis\n",
        "\n",
        "Analyze misclassifications and identify patterns in errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get best model predictions for error analysis\n",
        "best_model_name = all_metrics_df['auc'].idxmax()\n",
        "print(f\"Analyzing errors for best model: {best_model_name}\")\n",
        "\n",
        "if best_model_name in sequential_results:\n",
        "    # sequential model\n",
        "    model, test_loader, device, model_type = sequential_results[best_model_name]\n",
        "    y_true, y_pred, y_proba = evaluate_sequential_model(model, test_loader, device, model_type)\n",
        "else:\n",
        "    # baseline model\n",
        "    model, X_test, y_test, y_pred, y_proba = baseline_results[best_model_name]\n",
        "    y_true = y_test\n",
        "\n",
        "# confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# confusion matrix heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Normal', 'Fake'], yticklabels=['Normal', 'Fake'])\n",
        "axes[0].set_xlabel('Predicted', fontsize=12)\n",
        "axes[0].set_ylabel('True', fontsize=12)\n",
        "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "\n",
        "# error distribution\n",
        "errors = y_true != y_pred\n",
        "if y_proba.ndim > 1:\n",
        "    y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
        "else:\n",
        "    y_proba_positive = y_proba\n",
        "\n",
        "axes[1].hist(y_proba_positive[errors & (y_true == 0)], bins=20, \n",
        "            alpha=0.6, label='False Positives', color='red', density=True)\n",
        "axes[1].hist(y_proba_positive[errors & (y_true == 1)], bins=20, \n",
        "            alpha=0.6, label='False Negatives', color='orange', density=True)\n",
        "axes[1].set_xlabel('Prediction Score', fontsize=12)\n",
        "axes[1].set_ylabel('Density', fontsize=12)\n",
        "axes[1].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print error statistics\n",
        "print(f\"\\nError Statistics for {best_model_name}:\")\n",
        "print(f\"  Total errors: {errors.sum()}\")\n",
        "print(f\"  False Positives: {(errors & (y_true == 0)).sum()}\")\n",
        "print(f\"  False Negatives: {(errors & (y_true == 1)).sum()}\")\n",
        "print(f\"  Error rate: {errors.mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"MODEL SELECTION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# best model\n",
        "best_model_name = all_metrics_df['auc'].idxmax()\n",
        "best_metrics = all_metrics_df.loc[best_model_name]\n",
        "\n",
        "print(f\"\\nChampion Model: {best_model_name}\")\n",
        "print(f\"  Model Type: {best_metrics['model_type']}\")\n",
        "print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
        "print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
        "print(f\"  F1-Score: {best_metrics['f1']:.4f}\")\n",
        "print(f\"  False Positive Rate: {best_metrics['false_positive_rate']:.4f}\")\n",
        "\n",
        "# comparison baseline vs sequential\n",
        "baseline_avg_auc = all_metrics_df[all_metrics_df['model_type'] == 'baseline']['auc'].mean()\n",
        "sequential_avg_auc = all_metrics_df[all_metrics_df['model_type'] == 'sequential']['auc'].mean()\n",
        "\n",
        "print(f\"\\nAverage Performance:\")\n",
        "print(f\"  Baseline models: {baseline_avg_auc:.4f}\")\n",
        "print(f\"  Sequential models: {sequential_avg_auc:.4f}\")\n",
        "print(f\"  Improvement: {(sequential_avg_auc - baseline_avg_auc):.4f} ({(sequential_avg_auc - baseline_avg_auc) / baseline_avg_auc * 100:.1f}%)\")\n",
        "\n",
        "# top 3 models\n",
        "print(f\"\\nTop 3 Models (by AUC):\")\n",
        "top_3 = all_metrics_df.nlargest(3, 'auc')\n",
        "for idx, (model_name, row) in enumerate(top_3.iterrows(), 1):\n",
        "    print(f\"  {idx}. {model_name}: AUC={row['auc']:.4f}, F1={row['f1']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
