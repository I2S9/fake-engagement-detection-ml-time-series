{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "This notebook provides comprehensive evaluation of all trained models:\n",
    "- Baseline models (Logistic Regression, Random Forest, Isolation Forest, LOF)\n",
    "- Sequential models (LSTM, TCN, Autoencoder)\n",
    "\n",
    "It includes:\n",
    "- ROC curves comparison\n",
    "- Score distributions\n",
    "- Comprehensive metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline",
    "import sys",
    "from pathlib import Path",
    "",
    "# add project root to path",
    "project_root = Path().resolve().parent",
    "sys.path.insert(0, str(project_root))",
    "",
    "import pandas as pd",
    "import numpy as np",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "import torch",
    "from torch.utils.data import DataLoader",
    "from sklearn.metrics import precision_recall_curve, auc",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# set plotting style",
    "try:",
    "    plt.style.use('seaborn-v0_8-darkgrid')",
    "except OSError:",
    "    try:",
    "        plt.style.use('seaborn-darkgrid')",
    "    except OSError:",
    "        plt.style.use('default')",
    "sns.set_palette(\"husl\")",
    "plt.rcParams['figure.figsize'] = (14, 8)",
    "plt.rcParams['font.size'] = 10",
    "plt.rcParams['axes.labelsize'] = 12",
    "plt.rcParams['axes.titlesize'] = 14",
    "plt.rcParams['xtick.labelsize'] = 10",
    "plt.rcParams['ytick.labelsize'] = 10",
    "plt.rcParams['legend.fontsize'] = 10",
    "",
    "# create output directory for saved plots",
    "output_dir = project_root / \"outputs\" / \"figures\"",
    "output_dir.mkdir(parents=True, exist_ok=True)",
    "",
    "# import project modules",
    "from src.data.load_data import load_data",
    "from src.data.sequence_preparation import prepare_sequences_for_training",
    "from src.data.dataset import create_dataloaders_from_dict",
    "from src.features.temporal_features import extract_temporal_features",
    "from src.models.baselines import load_baseline_model",
    "from src.models.lstm import LSTMModel",
    "from src.models.tcn import TCNModel",
    "from src.models.autoencoder import AutoencoderModel",
    "from src.training.evaluate import (",
    "    evaluate_sequential_model,",
    "    compare_all_models,",
    "    plot_roc_curve,",
    "    plot_score_distributions,",
    "    compute_metrics,",
    ")",
    "from src.utils.config import load_config",
    "from src.visualization.plots import plot_score_with_threshold, plot_series_with_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models\n",
    "\n",
    "Load test data and all trained models for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config = load_config(project_root / \"config\" / \"config.yaml\")\n",
    "\n",
    "# load dataset\n",
    "data_path = project_root / \"data\" / \"raw\" / \"engagement.parquet\"\n",
    "df = load_data(data_path)\n",
    "\n",
    "# adapt column names if needed\n",
    "if 'user_id' in df.columns and 'id' not in df.columns:\n",
    "    df['id'] = df['user_id']\n",
    "if 'is_fake_series' in df.columns and 'label' not in df.columns:\n",
    "    df['label'] = df['is_fake_series'].map({True: 'fake', False: 'normal'})\n",
    "\n",
    "# prepare features for baseline models\n",
    "features_df = extract_temporal_features(df, aggregate_per_id=True)\n",
    "\n",
    "# prepare sequences for sequential models\n",
    "sequences_dict = prepare_sequences_for_training(\n",
    "    df,\n",
    "    seq_len=config[\"data\"][\"seq_len\"],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# create test dataloader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = create_dataloaders_from_dict(\n",
    "    sequences_dict,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    random_seed=42\n",
    ")\n",
    "test_loader = dataloaders[\"test\"]\n",
    "\n",
    "print(\"Data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline models\n",
    "baseline_results = {}\n",
    "baseline_dir = project_root / \"models\" / \"baselines\"\n",
    "\n",
    "for model_type in [\"logistic_regression\", \"random_forest\", \"isolation_forest\"]:\n",
    "    model_path = baseline_dir / f\"{model_type}.pkl\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            from src.models.baselines import create_baseline_model\n",
    "            model = create_baseline_model(model_type)\n",
    "            model.load(str(model_path))\n",
    "            from src.training.train import prepare_data\n",
    "            X_train, X_test, y_train, y_test, _ = prepare_data(features_df, test_size=0.2, random_state=42)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            baseline_results[model_type] = (model, X_test, y_test, y_pred, y_proba)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")\n",
    "\n",
    "# load sequential models\n",
    "sequential_results = {}\n",
    "sequential_dir = project_root / \"models\" / \"sequential\"\n",
    "\n",
    "for model_type in [\"lstm\", \"tcn\", \"autoencoder\"]:\n",
    "    model_path = sequential_dir / f\"{model_type}_best.pth\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(str(model_path), map_location=device, weights_only=False)\n",
    "            if model_type == \"lstm\":\n",
    "                model = LSTMModel(**config[\"models\"][\"lstm\"])\n",
    "            elif model_type == \"tcn\":\n",
    "                model = TCNModel(**config[\"models\"][\"tcn\"])\n",
    "            elif model_type == \"autoencoder\":\n",
    "                model = AutoencoderModel(**config[\"models\"][\"autoencoder\"], seq_len=config[\"data\"][\"seq_len\"])\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            sequential_results[model_type] = (model, test_loader, device, model_type)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(baseline_results)} baseline models and {len(sequential_results)} sequential models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Metrics Table\n",
    "\n",
    "Create comparison table of all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "all_metrics = {}\n",
    "\n",
    "# initialize if not loaded\n",
    "if 'baseline_results' not in locals():\n",
    "    baseline_results = {}\n",
    "if 'sequential_results' not in locals():\n",
    "    sequential_results = {}\n",
    "\n",
    "# baseline models\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba)\n",
    "    metrics[\"model_type\"] = \"baseline\"\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "# sequential models\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
    "    metrics[\"model_type\"] = \"sequential\"\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "# create DataFrame\n",
    "if len(all_metrics) > 0:\n",
    "    metrics_df = pd.DataFrame(all_metrics).T\n",
    "    display_metrics = [\"auc\", \"precision\", \"recall\", \"f1\", \"false_positive_rate\"]\n",
    "    metrics_display = metrics_df[display_metrics].copy()\n",
    "    metrics_display[\"model_type\"] = metrics_df[\"model_type\"]\n",
    "    metrics_display = metrics_display.sort_values(\"auc\", ascending=False)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nMetrics Table:\")\n",
    "    print(metrics_display.round(4))\n",
    "\n",
    "    # best model\n",
    "    best_model_name = metrics_display.index[0]\n",
    "    best_metrics = metrics_display.loc[best_model_name]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"BEST MODEL: {best_model_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
    "    print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {best_metrics['f1']:.4f}\")\n",
    "else:\n",
    "    print(\"No models loaded. Please load models first.\")\n",
    "    metrics_display = pd.DataFrame()\n",
    "    best_model_name = None\n",
    "    best_metrics = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions\n",
    "\n",
    "Final summary of evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# initialize variables if not defined\n",
    "if 'baseline_results' not in locals():\n",
    "    baseline_results = {}\n",
    "if 'sequential_results' not in locals():\n",
    "    sequential_results = {}\n",
    "if 'all_metrics' not in locals():\n",
    "    all_metrics = {}\n",
    "\n",
    "if len(all_metrics) > 0:\n",
    "    print(f\"\\nTotal models evaluated: {len(all_metrics)}\")\n",
    "    print(f\"  - Baseline models: {len(baseline_results)}\")\n",
    "    print(f\"  - Sequential models: {len(sequential_results)}\")\n",
    "else:\n",
    "    print(\"\\nNo models evaluated yet. Please run the metrics computation cell first.\")\n",
    "\n",
    "if ('all_metrics' in locals() and len(all_metrics) > 0 and \n",
    "    'best_model_name' in locals() and 'best_metrics' in locals()):\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    print(f\"  - AUC: {best_metrics['auc']:.4f}\")\n",
    "    print(f\"  - Precision: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"  - Recall: {best_metrics['recall']:.4f}\")\n",
    "    print(f\"  - F1-Score: {best_metrics['f1']:.4f}\")\n",
    "    print(f\"  - False Positive Rate: {best_metrics['false_positive_rate']:.4f}\")\n",
    "    \n",
    "    # model type comparison\n",
    "    if 'metrics_display' in locals() and 'display_metrics' in locals():\n",
    "        baseline_models = metrics_display[metrics_display['model_type'] == 'baseline']\n",
    "        sequential_models = metrics_display[metrics_display['model_type'] == 'sequential']\n",
    "        \n",
    "        if len(baseline_models) > 0 and len(sequential_models) > 0:\n",
    "            baseline_avg = baseline_models[display_metrics].mean()\n",
    "            sequential_avg = sequential_models[display_metrics].mean()\n",
    "            \n",
    "            print(f\"\\nAverage Performance by Model Type:\")\n",
    "            print(f\"  Baseline models:\")\n",
    "            for metric in display_metrics:\n",
    "                print(f\"    {metric}: {baseline_avg[metric]:.4f}\")\n",
    "            print(f\"  Sequential models:\")\n",
    "            for metric in display_metrics:\n",
    "                print(f\"    {metric}: {sequential_avg[metric]:.4f}\")\n",
    "            \n",
    "            improvement = sequential_avg['auc'] - baseline_avg['auc']\n",
    "            print(f\"\\n  Improvement (Sequential vs Baseline):\")\n",
    "            print(f\"    AUC: {improvement:.4f} ({improvement / baseline_avg['auc'] * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequential models\n",
    "sequential_results = {}\n",
    "sequential_dir = project_root / \"models\" / \"sequential\"\n",
    "\n",
    "for model_type in [\"lstm\", \"tcn\", \"autoencoder\"]:\n",
    "    model_path = sequential_dir / f\"{model_type}_best.pth\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(str(model_path), map_location=device, weights_only=False)\n",
    "            if model_type == \"lstm\":\n",
    "                model = LSTMModel(**config[\"models\"][\"lstm\"])\n",
    "            elif model_type == \"tcn\":\n",
    "                model = TCNModel(**config[\"models\"][\"tcn\"])\n",
    "            elif model_type == \"autoencoder\":\n",
    "                model = AutoencoderModel(**config[\"models\"][\"autoencoder\"], seq_len=config[\"data\"][\"seq_len\"])\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            sequential_results[model_type] = (model, test_loader, device, model_type)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROC Curves\n",
    "\n",
    "Plot ROC curves for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# ROC curves\n",
    "ax = axes[0]\n",
    "# baseline models\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    plot_roc_curve(y_test, y_proba, model_name=model_name, ax=ax)\n",
    "\n",
    "# sequential models\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    plot_roc_curve(y_true, y_proba, model_name=model_name, ax=ax)\n",
    "\n",
    "ax.set_title(\"ROC Curves - All Models\", fontsize=16, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# PR curves\n",
    "ax = axes[1]\n",
    "# baseline models\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba_positive)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    ax.plot(recall, precision, label=f'{model_name} (AUC={pr_auc:.3f})', linewidth=2)\n",
    "\n",
    "# sequential models\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba_positive)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    ax.plot(recall, precision, label=f'{model_name} (AUC={pr_auc:.3f})', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title(\"Precision-Recall Curves - All Models\", fontsize=16, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"04_roc_pr_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Distributions\n",
    "\n",
    "Visualize score distributions for normal vs fake classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect predictions\n",
    "all_predictions = {}\n",
    "\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    all_predictions[model_name] = (y_test, y_proba_positive)\n",
    "\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    all_predictions[model_name] = (y_true, y_proba_positive)\n",
    "\n",
    "# plot distributions\n",
    "n_models = len(all_predictions)\n",
    "fig, axes = plt.subplots((n_models + 1) // 2, 2, figsize=(16, 4 * ((n_models + 1) // 2)))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, (y_true, y_proba)) in enumerate(all_predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    normal_scores = y_proba[y_true == 0]\n",
    "    fake_scores = y_proba[y_true == 1]\n",
    "    ax.hist(normal_scores, bins=50, alpha=0.6, label=\"Normal\", color=\"blue\", density=True, histtype=\"step\", linewidth=2)\n",
    "    ax.hist(fake_scores, bins=50, alpha=0.6, label=\"Fake\", color=\"red\", density=True, histtype=\"step\", linewidth=2, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Prediction Score\", fontsize=12)\n",
    "    ax.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax.set_title(f\"Score Distribution - {model_name.upper()}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0.5, color=\"gray\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"04_score_distributions.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics Heatmap Comparison\n",
    "\n",
    "Create a comprehensive heatmap comparing all models across all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Score Distributions by Attack Type\n",
    "\n",
    "Compare score distributions across different attack types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score distributions by attack type\n",
    "if 'attack_type' in df.columns and len(sequential_results) > 0:\n",
    "    # get best model predictions\n",
    "    best_model_name = max(sequential_results.keys(), key=lambda k: len(sequential_results[k]))\n",
    "    model, dataloader, device, model_type = sequential_results[best_model_name]\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    \n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    \n",
    "    # get attack types for fake samples (simplified mapping)\n",
    "    fake_df = df[df.get('is_fake_series', df.get('label') == 'fake')]\n",
    "    attack_types = fake_df['attack_type'].unique()\n",
    "    \n",
    "    if len(attack_types) > 0:\n",
    "        fake_mask = y_true == 1\n",
    "        fake_scores = y_proba_positive[fake_mask]\n",
    "        \n",
    "        # create distribution plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # histogram by attack type\n",
    "        for attack_type in attack_types:\n",
    "            # simplified: sample evenly from fake scores\n",
    "            n_fake = len(fake_scores)\n",
    "            n_types = len(attack_types)\n",
    "            samples_per_type = n_fake // n_types\n",
    "            idx = list(attack_types).index(attack_type)\n",
    "            start_idx = idx * samples_per_type\n",
    "            end_idx = start_idx + samples_per_type if idx < n_types - 1 else n_fake\n",
    "            type_scores = fake_scores[start_idx:end_idx]\n",
    "            \n",
    "            axes[0].hist(type_scores, bins=30, alpha=0.6, label=attack_type, density=True)\n",
    "        \n",
    "        axes[0].set_xlabel('Prediction Score', fontsize=12)\n",
    "        axes[0].set_ylabel('Density', fontsize=12)\n",
    "        axes[0].set_title('Score Distribution by Attack Type', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].axvline(x=0.5, color='gray', linestyle=':', linewidth=1)\n",
    "        \n",
    "        # box plot\n",
    "        score_data = []\n",
    "        labels = []\n",
    "        for attack_type in attack_types:\n",
    "            idx = list(attack_types).index(attack_type)\n",
    "            start_idx = idx * samples_per_type\n",
    "            end_idx = start_idx + samples_per_type if idx < n_types - 1 else n_fake\n",
    "            type_scores = fake_scores[start_idx:end_idx]\n",
    "            score_data.append(type_scores)\n",
    "            labels.append(attack_type)\n",
    "        \n",
    "        axes[1].boxplot(score_data, labels=labels)\n",
    "        axes[1].set_ylabel('Prediction Score', fontsize=12)\n",
    "        axes[1].set_title('Score Distribution by Attack Type (Box Plot)', fontsize=14, fontweight='bold')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        axes[1].axhline(y=0.5, color='gray', linestyle=':', linewidth=1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / \"04_scores_by_attack_type.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No attack types found in dataset.\")\n",
    "else:\n",
    "    print(\"Attack type column not found or no sequential models available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Scores with Thresholds\n",
    "\n",
    "Visualize prediction scores over time with threshold lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal scores with thresholds\n",
    "if len(sequential_results) > 0:\n",
    "    # get a sample user series\n",
    "    sample_user_id = df['id'].unique()[0]\n",
    "    sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "    \n",
    "    # get model predictions (simplified - would need proper sequence mapping)\n",
    "    best_model_name = max(sequential_results.keys(), key=lambda k: len(sequential_results[k]))\n",
    "    model, dataloader, device, model_type = sequential_results[best_model_name]\n",
    "    \n",
    "    # create dummy scores for visualization (in real scenario, would get per-timestep scores)\n",
    "    n_timesteps = len(sample_series)\n",
    "    dummy_scores = np.random.uniform(0.3, 0.7, n_timesteps)\n",
    "    # add some spikes for fake series\n",
    "    if sample_series.get('is_fake_series', pd.Series([False] * len(sample_series))).any():\n",
    "        spike_indices = np.random.choice(n_timesteps, size=min(5, n_timesteps // 10), replace=False)\n",
    "        dummy_scores[spike_indices] = np.random.uniform(0.7, 0.95, len(spike_indices))\n",
    "    \n",
    "    threshold = 0.5\n",
    "    \n",
    "    fig, ax = plot_score_with_threshold(\n",
    "        sample_series['timestamp'],\n",
    "        dummy_scores,\n",
    "        threshold,\n",
    "        title=f\"Temporal Anomaly Scores - User: {sample_user_id}\"\n",
    "    )\n",
    "    plt.savefig(output_dir / \"04_temporal_scores_threshold.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sequential models available for temporal score visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predicted vs True Anomalies\n",
    "\n",
    "Compare predicted anomalies with true anomaly windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted vs true anomalies\n",
    "if 'is_anomaly_window' in df.columns and len(sequential_results) > 0:\n",
    "    # select a fake user series\n",
    "    fake_users = df[df.get('is_fake_series', df.get('label') == 'fake')]['id'].unique()\n",
    "    if len(fake_users) > 0:\n",
    "        sample_user_id = fake_users[0]\n",
    "        sample_series = df[df['id'] == sample_user_id].sort_values('timestamp')\n",
    "        \n",
    "        # true anomaly mask\n",
    "        true_anomaly_mask = sample_series.get('is_anomaly_window', pd.Series([False] * len(sample_series))).values\n",
    "        \n",
    "        # predicted anomaly mask (simplified - would need proper model predictions per timestep)\n",
    "        # create dummy predictions based on views spikes\n",
    "        views = sample_series['views'].values\n",
    "        views_mean = views.mean()\n",
    "        views_std = views.std()\n",
    "        predicted_anomaly_mask = (views > views_mean + 2 * views_std) | (views < views_mean - 2 * views_std)\n",
    "        \n",
    "        # plot comparison\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "        \n",
    "        # top: original series with true anomalies\n",
    "        ax1 = axes[0]\n",
    "        ax1.plot(sample_series['timestamp'], sample_series['views'], \n",
    "                label='Views', linewidth=2, color='blue', alpha=0.7)\n",
    "        if true_anomaly_mask.any():\n",
    "            ax1.scatter(\n",
    "                sample_series['timestamp'][true_anomaly_mask],\n",
    "                sample_series['views'].values[true_anomaly_mask],\n",
    "                marker='o', s=50, color='red', label='True Anomaly', zorder=5\n",
    "            )\n",
    "        ax1.set_ylabel('Views', fontsize=12)\n",
    "        ax1.set_title(f'True Anomalies - User: {sample_user_id}', fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # bottom: original series with predicted anomalies\n",
    "        ax2 = axes[1]\n",
    "        ax2.plot(sample_series['timestamp'], sample_series['views'], \n",
    "                label='Views', linewidth=2, color='blue', alpha=0.7)\n",
    "        if predicted_anomaly_mask.any():\n",
    "            ax2.scatter(\n",
    "                sample_series['timestamp'][predicted_anomaly_mask],\n",
    "                sample_series['views'].values[predicted_anomaly_mask],\n",
    "                marker='s', s=50, color='orange', label='Predicted Anomaly', zorder=5\n",
    "            )\n",
    "        if true_anomaly_mask.any():\n",
    "            # highlight overlap\n",
    "            overlap_mask = true_anomaly_mask & predicted_anomaly_mask\n",
    "            if overlap_mask.any():\n",
    "                ax2.scatter(\n",
    "                    sample_series['timestamp'][overlap_mask],\n",
    "                    sample_series['views'].values[overlap_mask],\n",
    "                    marker='*', s=100, color='green', label='Correctly Predicted', zorder=6\n",
    "                )\n",
    "        ax2.set_xlabel('Timestamp', fontsize=12)\n",
    "        ax2.set_ylabel('Views', fontsize=12)\n",
    "        ax2.set_title(f'Predicted Anomalies - User: {sample_user_id}', fontsize=14, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / \"04_predicted_vs_true_anomalies.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # print statistics\n",
    "        if true_anomaly_mask.any():\n",
    "            tp = (true_anomaly_mask & predicted_anomaly_mask).sum()\n",
    "            fp = (~true_anomaly_mask & predicted_anomaly_mask).sum()\n",
    "            fn = (true_anomaly_mask & ~predicted_anomaly_mask).sum()\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            print(f\"\\nAnomaly Detection Statistics for User {sample_user_id}:\")\n",
    "            print(f\"  True Positives: {tp}\")\n",
    "            print(f\"  False Positives: {fp}\")\n",
    "            print(f\"  False Negatives: {fn}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "    else:\n",
    "        print(\"No fake users found in dataset.\")\n",
    "else:\n",
    "    print(\"Required columns (is_anomaly_window) not found or no sequential models available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create comprehensive metrics heatmap\n",
    "if ('all_metrics' in locals() and 'metrics_display' in locals() and \n",
    "    'display_metrics' in locals() and len(all_metrics) > 0):\n",
    "    # prepare data for heatmap\n",
    "    heatmap_data = metrics_display[display_metrics].T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(max(10, len(all_metrics) * 1.5), 6))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Score'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Model Performance Heatmap - All Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Metric', fontsize=12)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"04_metrics_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # create comparison bar chart\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(len(display_metrics))\n",
    "    width = 0.8 / len(metrics_display)\n",
    "    \n",
    "    colors_map = {'baseline': 'blue', 'sequential': 'red'}\n",
    "    for idx, (model_name, row) in enumerate(metrics_display.iterrows()):\n",
    "        values = [row[m] for m in display_metrics]\n",
    "        color = colors_map.get(row['model_type'], 'gray')\n",
    "        ax.bar(x + idx * width, values, width, label=model_name, \n",
    "               alpha=0.7, color=color, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Metric', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Model Performance Comparison - All Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x + width * (len(metrics_display) - 1) / 2)\n",
    "    ax.set_xticklabels(display_metrics)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"04_metrics_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models loaded. Please load models first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}