{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "This notebook provides comprehensive evaluation of all trained models:\n",
    "- Baseline models (Logistic Regression, Random Forest, Isolation Forest, LOF)\n",
    "- Sequential models (LSTM, TCN, Autoencoder)\n",
    "\n",
    "It includes:\n",
    "- ROC curves comparison\n",
    "- Score distributions\n",
    "- Comprehensive metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# import project modules\n",
    "from src.data.preprocess import load_and_preprocess\n",
    "from src.data.sequence_preparation import prepare_sequences_for_training\n",
    "from src.data.dataset import create_dataloaders_from_dict\n",
    "from src.features.temporal_features import extract_temporal_features\n",
    "from src.models.baselines import load_baseline_model\n",
    "from src.models.lstm import LSTMModel\n",
    "from src.models.tcn import TCNModel\n",
    "from src.models.autoencoder import AutoencoderModel\n",
    "from src.training.evaluate import (\n",
    "    evaluate_sequential_model,\n",
    "    compare_all_models,\n",
    "    plot_roc_curve,\n",
    "    plot_score_distributions,\n",
    "    compute_metrics,\n",
    ")\n",
    "from src.utils.config import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models\n",
    "\n",
    "Load test data and all trained models for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config = load_config(project_root / \"config\" / \"config.yaml\")\n",
    "\n",
    "# load data\n",
    "data_path = project_root / \"data\" / \"raw\" / \"engagement_timeseries.parquet\"\n",
    "df = load_and_preprocess(\n",
    "    file_path=str(data_path),\n",
    "    target_timezone=\"UTC\",\n",
    "    resample_frequency=\"h\",\n",
    "    handle_missing=True,\n",
    "    missing_method=\"forward\",\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# prepare features for baseline models\n",
    "features_df = extract_temporal_features(df, aggregate_per_id=True)\n",
    "\n",
    "# prepare sequences for sequential models\n",
    "sequences_dict = prepare_sequences_for_training(\n",
    "    df,\n",
    "    seq_len=config[\"data\"][\"seq_len\"],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# create test dataloader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = create_dataloaders_from_dict(\n",
    "    sequences_dict,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    random_seed=42\n",
    ")\n",
    "test_loader = dataloaders[\"test\"]\n",
    "\n",
    "print(\"Data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline models\n",
    "baseline_results = {}\n",
    "baseline_dir = project_root / \"models\" / \"baselines\"\n",
    "\n",
    "for model_type in [\"logistic_regression\", \"random_forest\", \"isolation_forest\"]:\n",
    "    model_path = baseline_dir / f\"{model_type}.pkl\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            model = load_baseline_model(str(model_path))\n",
    "            from src.training.train import prepare_data\n",
    "            X_train, X_test, y_train, y_test, _ = prepare_data(features_df, test_size=0.2, random_state=42)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            baseline_results[model_type] = (model, X_test, y_test, y_pred, y_proba)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequential models\n",
    "sequential_results = {}\n",
    "sequential_dir = project_root / \"models\" / \"sequential\"\n",
    "\n",
    "for model_type in [\"lstm\", \"tcn\", \"autoencoder\"]:\n",
    "    model_path = sequential_dir / f\"{model_type}_best.pth\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(str(model_path), map_location=device, weights_only=False)\n",
    "            if model_type == \"lstm\":\n",
    "                model = LSTMModel(**config[\"models\"][\"lstm\"])\n",
    "            elif model_type == \"tcn\":\n",
    "                model = TCNModel(**config[\"models\"][\"tcn\"])\n",
    "            elif model_type == \"autoencoder\":\n",
    "                model = AutoencoderModel(**config[\"models\"][\"autoencoder\"], seq_len=config[\"data\"][\"seq_len\"])\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            sequential_results[model_type] = (model, test_loader, device, model_type)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROC Curves\n",
    "\n",
    "Plot ROC curves for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# baseline models\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    plot_roc_curve(y_test, y_proba, model_name=model_name, ax=ax)\n",
    "\n",
    "# sequential models\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    plot_roc_curve(y_true, y_proba, model_name=model_name, ax=ax)\n",
    "\n",
    "ax.set_title(\"ROC Curves - All Models\", fontsize=16, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Distributions\n",
    "\n",
    "Visualize score distributions for normal vs fake classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect predictions\n",
    "all_predictions = {}\n",
    "\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    all_predictions[model_name] = (y_test, y_proba_positive)\n",
    "\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    all_predictions[model_name] = (y_true, y_proba_positive)\n",
    "\n",
    "# plot distributions\n",
    "n_models = len(all_predictions)\n",
    "fig, axes = plt.subplots((n_models + 1) // 2, 2, figsize=(16, 4 * ((n_models + 1) // 2)))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, (y_true, y_proba)) in enumerate(all_predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    normal_scores = y_proba[y_true == 0]\n",
    "    fake_scores = y_proba[y_true == 1]\n",
    "    ax.hist(normal_scores, bins=50, alpha=0.6, label=\"Normal\", color=\"blue\", density=True, histtype=\"step\", linewidth=2)\n",
    "    ax.hist(fake_scores, bins=50, alpha=0.6, label=\"Fake\", color=\"red\", density=True, histtype=\"step\", linewidth=2, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Prediction Score\", fontsize=12)\n",
    "    ax.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax.set_title(f\"Score Distribution - {model_name.upper()}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0.5, color=\"gray\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Metrics Table\n",
    "\n",
    "Create comparison table of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba)\n",
    "    metrics[\"model_type\"] = \"baseline\"\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
    "    metrics[\"model_type\"] = \"sequential\"\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "# create DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "display_metrics = [\"auc\", \"precision\", \"recall\", \"f1\", \"false_positive_rate\"]\n",
    "metrics_display = metrics_df[display_metrics].copy()\n",
    "metrics_display[\"model_type\"] = metrics_df[\"model_type\"]\n",
    "metrics_display = metrics_display.sort_values(\"auc\", ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nMetrics Table:\")\n",
    "print(metrics_display.round(4))\n",
    "\n",
    "# best model\n",
    "best_model_name = metrics_display.index[0]\n",
    "best_metrics = metrics_display.loc[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEST MODEL: {best_model_name.upper()}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
    "print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {best_metrics['f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}