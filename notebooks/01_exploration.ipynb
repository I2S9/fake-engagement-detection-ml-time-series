{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Engagement Time Series\n",
    "\n",
    "This notebook explores the synthetic engagement time series data to understand:\n",
    "- Distribution of engagement metrics (views, likes, comments, shares)\n",
    "- Differences between normal and fake engagement patterns\n",
    "- Temporal patterns and anomalies\n",
    "- Class balance and data quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# create output directory for plots\n",
    "output_dir = project_root / \"outputs\" / \"figures\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set plotting style - FORCE DISPLAY\n",
    "plt.ion()  # interactive mode\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# import project modules\n",
    "from src.data.load_data import load_data\n",
    "from src.data.make_dataset import main as generate_dataset_main\n",
    "from src.visualization.plots import plot_series_with_anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate and Load Data\n",
    "\n",
    "First, we generate a synthetic dataset with normal and fake engagement patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = project_root / \"data\" / \"raw\" / \"engagement.parquet\"\n",
    "\n",
    "# generate if it doesn't exist\n",
    "if not data_path.exists():\n",
    "    print(\"Generating dataset...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\n",
    "        \"python\", \"-m\", \"src.data.make_dataset\",\n",
    "        \"--n_users\", \"500\",\n",
    "        \"--length\", \"336\",\n",
    "        \"--fake_ratio\", \"0.35\"\n",
    "    ], cwd=str(project_root))\n",
    "    print(\"Dataset generated!\")\n",
    "else:\n",
    "    print(f\"Loading existing dataset from {data_path}\")\n",
    "\n",
    "# load dataset\n",
    "df = load_data(data_path)\n",
    "\n",
    "# adapt column names if needed\n",
    "if 'user_id' in df.columns and 'id' not in df.columns:\n",
    "    df['id'] = df['user_id']\n",
    "if 'is_fake_series' in df.columns and 'label' not in df.columns:\n",
    "    df['label'] = df['is_fake_series'].map({True: 'fake', False: 'normal'})\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "if 'is_fake_series' in df.columns:\n",
    "    print(f\"\\nFake series distribution:\")\n",
    "    print(df['is_fake_series'].value_counts())\n",
    "    print(f\"Fake ratio: {df['is_fake_series'].mean():.2%}\")\n",
    "if 'label' in df.columns:\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "print(f\"\\nNumber of unique users: {df['id'].nunique()}\")\n",
    "if 'profile' in df.columns:\n",
    "    print(f\"\\nProfile distribution:\")\n",
    "    print(df['profile'].value_counts())\n",
    "if 'attack_type' in df.columns:\n",
    "    print(f\"\\nAttack type distribution (fake only):\")\n",
    "    fake_df = df[df.get('is_fake_series', df.get('label') == 'fake')]\n",
    "    if len(fake_df) > 0:\n",
    "        print(fake_df['attack_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "Basic statistics and data quality checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "metrics = ['views', 'likes', 'comments', 'shares']\n",
    "\n",
    "# ensure label column exists\n",
    "is_fake_col = df.get('is_fake_series', None)\n",
    "if is_fake_col is None:\n",
    "    is_fake_col = (df.get('label', pd.Series(['normal'] * len(df))) == 'fake')\n",
    "if 'label' not in df.columns:\n",
    "    df['label'] = is_fake_col.map({True: 'fake', False: 'normal'})\n",
    "\n",
    "# basic statistics by label\n",
    "print(\"Summary statistics by label:\\n\")\n",
    "print(df.groupby('label')[metrics].describe())\n",
    "\n",
    "# check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df[metrics].isna().sum())\n",
    "\n",
    "# check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df[metrics + ['timestamp']].dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis - Histograms\n",
    "\n",
    "Compare the distribution of engagement metrics between normal and fake patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histograms for each metric\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # plot histograms for normal and fake\n",
    "    normal_data = df[df['label'] == 'normal'][metric].dropna()\n",
    "    fake_data = df[df['label'] == 'fake'][metric].dropna()\n",
    "    \n",
    "    if len(normal_data) > 0 and len(fake_data) > 0:\n",
    "        ax.hist(normal_data, bins=50, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "        ax.hist(fake_data, bins=50, alpha=0.6, label='Fake', color='red', density=True)\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    ax.set_xlabel(metric.capitalize(), fontsize=12)\n",
    "    ax.set_ylabel('Density (log scale)', fontsize=12)\n",
    "    ax.set_title(f'Distribution of {metric.capitalize()}', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_01_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Graphique sauvegarde: 01_exploration_01_plot.png\")\n",
    "\n",
    "# print statistics\n",
    "print(\"\\nMean values by label:\")\n",
    "print(df.groupby('label')[metrics].mean())\n",
    "print(\"\\nMedian values by label:\")\n",
    "print(df.groupby('label')[metrics].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Average Temporal Patterns\n",
    "\n",
    "Compare average engagement curves over time for normal vs fake patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize time for each video (0 to 1)\n",
    "df_normalized_time = df.copy()\n",
    "df_normalized_time['time_normalized'] = df_normalized_time.groupby('id')['timestamp'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0\n",
    ")\n",
    "\n",
    "# create time bins for averaging\n",
    "n_bins = 50\n",
    "df_normalized_time['time_bin'] = pd.cut(df_normalized_time['time_normalized'], bins=n_bins, labels=False)\n",
    "\n",
    "# calculate average curves by label\n",
    "avg_curves = df_normalized_time.groupby(['label', 'time_bin'])[metrics].mean().reset_index()\n",
    "\n",
    "# plot average curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    normal_curve = avg_curves[avg_curves['label'] == 'normal']\n",
    "    fake_curve = avg_curves[avg_curves['label'] == 'fake']\n",
    "    \n",
    "    ax.plot(normal_curve['time_bin'], normal_curve[metric], \n",
    "            label='Normal', linewidth=2, color='blue', marker='o', markersize=4)\n",
    "    ax.plot(fake_curve['time_bin'], fake_curve[metric], \n",
    "            label='Fake', linewidth=2, color='red', marker='s', markersize=4)\n",
    "    \n",
    "    ax.set_xlabel('Normalized Time (0 = start, 1 = end)', fontsize=12)\n",
    "    ax.set_ylabel(f'Average {metric.capitalize()}', fontsize=12)\n",
    "    ax.set_title(f'Average {metric.capitalize()} Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_02_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Overview\n",
    "\n",
    "Basic statistics and data quality checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "metrics = ['views', 'likes', 'comments', 'shares']\n",
    "\n",
    "# basic statistics by label\n",
    "is_fake_col = df.get('is_fake_series', None)\n",
    "if is_fake_col is None:\n",
    "    is_fake_col = (df.get('label', pd.Series(['normal'] * len(df))) == 'fake')\n",
    "df['label'] = is_fake_col.map({True: 'fake', False: 'normal'})\n",
    "\n",
    "print(\"Summary statistics by label:\\n\")\n",
    "print(df.groupby('label')[metrics].describe())\n",
    "\n",
    "# check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df[metrics].isna().sum())\n",
    "\n",
    "# check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df[metrics + ['timestamp']].dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select example videos\n",
    "normal_ids = df[df['label'] == 'normal']['id'].unique()[:3]\n",
    "fake_ids = df[df['label'] == 'fake']['id'].unique()[:3]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# plot normal examples\n",
    "for idx, video_id in enumerate(normal_ids):\n",
    "    ax = axes[idx, 0]\n",
    "    video_data = df[df['id'] == video_id].sort_values('timestamp')\n",
    "    \n",
    "    ax.plot(video_data['timestamp'], video_data['views'], \n",
    "            label='Views', linewidth=2, color='blue')\n",
    "    ax.plot(video_data['timestamp'], video_data['likes'] * 20, \n",
    "            label='Likes (x20)', linewidth=2, color='green')\n",
    "    ax.plot(video_data['timestamp'], video_data['comments'] * 50, \n",
    "            label='Comments (x50)', linewidth=2, color='orange')\n",
    "    \n",
    "    ax.set_title(f'Normal Video: {video_id}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Timestamp', fontsize=10)\n",
    "    ax.set_ylabel('Engagement', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# plot fake examples\n",
    "for idx, video_id in enumerate(fake_ids):\n",
    "    ax = axes[idx, 1]\n",
    "    video_data = df[df['id'] == video_id].sort_values('timestamp')\n",
    "    \n",
    "    ax.plot(video_data['timestamp'], video_data['views'], \n",
    "            label='Views', linewidth=2, color='blue')\n",
    "    ax.plot(video_data['timestamp'], video_data['likes'] * 20, \n",
    "            label='Likes (x20)', linewidth=2, color='green')\n",
    "    ax.plot(video_data['timestamp'], video_data['comments'] * 50, \n",
    "            label='Comments (x50)', linewidth=2, color='orange')\n",
    "    \n",
    "    ax.set_title(f'Fake Video: {video_id}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Timestamp', fontsize=10)\n",
    "    ax.set_ylabel('Engagement', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_03_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pattern Distinctness Analysis\n",
    "\n",
    "Analyze how distinct fake patterns are from normal patterns using statistical measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Engagement Metrics Comparison Heatmap\n",
    "\n",
    "Compare mean engagement metrics between normal and fake patterns using a heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create comparison heatmap\n",
    "comparison_data = df.groupby('label')[metrics].agg(['mean', 'std', 'max', 'min']).T\n",
    "comparison_data.columns = [f\"{col[0]}_{col[1]}\" for col in comparison_data.columns]\n",
    "\n",
    "# create mean values heatmap\n",
    "mean_comparison = df.groupby('label')[metrics].mean().T\n",
    "mean_comparison.columns = ['Normal', 'Fake']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# absolute values\n",
    "sns.heatmap(mean_comparison, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Mean Value'}, ax=axes[0], linewidths=0.5)\n",
    "axes[0].set_title('Mean Engagement Metrics by Label', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Metric', fontsize=12)\n",
    "\n",
    "# normalized (fake/normal ratio)\n",
    "ratio_comparison = (mean_comparison['Fake'] / (mean_comparison['Normal'] + 1e-6)).to_frame('Fake/Normal Ratio')\n",
    "sns.heatmap(ratio_comparison, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
    "            center=1, vmin=0.5, vmax=2, cbar_kws={'label': 'Ratio'}, \n",
    "            ax=axes[1], linewidths=0.5)\n",
    "axes[1].set_title('Fake/Normal Ratio by Metric', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Metric', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_04_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean values comparison:\")\n",
    "print(mean_comparison)\n",
    "print(\"\\nFake/Normal ratios:\")\n",
    "print(ratio_comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate per-video statistics\n",
    "video_stats = df.groupby(['id', 'label']).agg({\n",
    "    'views': ['mean', 'std', 'max', 'min'],\n",
    "    'likes': ['mean', 'std', 'max', 'min'],\n",
    "    'comments': ['mean', 'std', 'max', 'min'],\n",
    "    'shares': ['mean', 'std', 'max', 'min']\n",
    "}).reset_index()\n",
    "\n",
    "video_stats.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in video_stats.columns.values]\n",
    "\n",
    "# calculate coefficient of variation (std/mean) as a measure of variability\n",
    "for metric in metrics:\n",
    "    mean_col = f'{metric}_mean'\n",
    "    std_col = f'{metric}_std'\n",
    "    cv_col = f'{metric}_cv'\n",
    "    video_stats[cv_col] = video_stats[std_col] / (video_stats[mean_col] + 1e-6)\n",
    "\n",
    "# compare statistics between normal and fake\n",
    "print(\"Per-video statistics comparison:\\n\")\n",
    "comparison_cols = [col for col in video_stats.columns if any(m in col for m in metrics) and ('mean' in col or 'cv' in col)]\n",
    "print(video_stats.groupby('label')[comparison_cols].mean())\n",
    "\n",
    "# plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    normal_cv = video_stats[video_stats['label'] == 'normal'][f'{metric}_cv']\n",
    "    fake_cv = video_stats[video_stats['label'] == 'fake'][f'{metric}_cv']\n",
    "    \n",
    "    ax.boxplot([normal_cv.dropna(), fake_cv.dropna()], \n",
    "               labels=['Normal', 'Fake'])\n",
    "    ax.set_ylabel('Coefficient of Variation', fontsize=12)\n",
    "    ax.set_title(f'{metric.capitalize()} Variability (CV)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_05_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Heatmaps - Normal vs Fake\n",
    "\n",
    "Compare correlation patterns between engagement metrics for normal and fake patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrices\n",
    "normal_df = df[df['label'] == 'normal'][metrics]\n",
    "fake_df = df[df['label'] == 'fake'][metrics]\n",
    "\n",
    "normal_corr = normal_df.corr()\n",
    "fake_corr = fake_df.corr()\n",
    "\n",
    "# plot correlation heatmaps side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# normal correlation heatmap\n",
    "sns.heatmap(normal_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, vmin=-1, vmax=1, square=True, ax=axes[0],\n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Normal Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].set_yticklabels(axes[0].get_yticklabels(), rotation=0)\n",
    "\n",
    "# fake correlation heatmap\n",
    "sns.heatmap(fake_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, vmin=-1, vmax=1, square=True, ax=axes[1],\n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "axes[1].set_title('Fake Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].set_yticklabels(axes[1].get_yticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_06_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# difference heatmap\n",
    "corr_diff = fake_corr - normal_corr\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(corr_diff, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, vmin=-0.5, vmax=0.5, square=True, ax=ax,\n",
    "            cbar_kws={'label': 'Difference (Fake - Normal)'})\n",
    "ax.set_title('Correlation Difference: Fake - Normal', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_07_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation differences (Fake - Normal):\")\n",
    "print(corr_diff.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Spectrogramme / FFT - Détection de Patterns Réguliers\n",
    "\n",
    "Analyse fréquentielle pour détecter les patterns réguliers (bots synchronisés).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT analysis for detecting regular patterns (bot-like behavior)\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# select example series\n",
    "normal_sample = df[df['label'] == 'normal']['id'].unique()[0]\n",
    "fake_sample = df[df['label'] == 'fake']['id'].unique()[0]\n",
    "\n",
    "normal_series = df[df['id'] == normal_sample].sort_values('timestamp')['views'].values\n",
    "fake_series = df[df['id'] == fake_sample].sort_values('timestamp')['views'].values\n",
    "\n",
    "# compute FFT\n",
    "normal_fft = np.abs(fft(normal_series))\n",
    "fake_fft = np.abs(fft(fake_series))\n",
    "freqs = fftfreq(len(normal_series), 1.0)  # assuming hourly data\n",
    "\n",
    "# plot FFT comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# time series\n",
    "axes[0, 0].plot(normal_series, label='Normal', linewidth=2, color='blue')\n",
    "axes[0, 0].set_title('Normal Series - Time Domain', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (hours)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Views', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(fake_series, label='Fake', linewidth=2, color='red')\n",
    "axes[0, 1].set_title('Fake Series - Time Domain', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (hours)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Views', fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# FFT magnitude\n",
    "positive_freqs = freqs[:len(freqs)//2]\n",
    "axes[1, 0].plot(positive_freqs, normal_fft[:len(normal_fft)//2], \n",
    "                label='Normal', linewidth=2, color='blue')\n",
    "axes[1, 0].set_title('Normal Series - Frequency Domain (FFT)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Frequency (1/hours)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Magnitude', fontsize=12)\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(positive_freqs, fake_fft[:len(fake_fft)//2], \n",
    "                label='Fake', linewidth=2, color='red')\n",
    "axes[1, 1].set_title('Fake Series - Frequency Domain (FFT)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Frequency (1/hours)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Magnitude', fontsize=12)\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_08_fft.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# spectrogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "f_normal, t_normal, Sxx_normal = signal.spectrogram(normal_series, nperseg=min(64, len(normal_series)//4))\n",
    "im1 = axes[0].pcolormesh(t_normal, f_normal, 10 * np.log10(Sxx_normal + 1e-10), shading='gouraud', cmap='viridis')\n",
    "axes[0].set_title('Normal Series - Spectrogram', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "plt.colorbar(im1, ax=axes[0], label='Power (dB)')\n",
    "\n",
    "f_fake, t_fake, Sxx_fake = signal.spectrogram(fake_series, nperseg=min(64, len(fake_series)//4))\n",
    "im2 = axes[1].pcolormesh(t_fake, f_fake, 10 * np.log10(Sxx_fake + 1e-10), shading='gouraud', cmap='viridis')\n",
    "axes[1].set_title('Fake Series - Spectrogram', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "plt.colorbar(im2, ax=axes[1], label='Power (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_09_spectrogram.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Distribution des Amplitudes\n",
    "\n",
    "Analyse de la distribution des amplitudes pour distinguer normal vs fake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution des amplitudes (max - min par série)\n",
    "amplitude_stats = df.groupby(['id', 'label']).agg({\n",
    "    'views': ['max', 'min', lambda x: x.max() - x.min()],\n",
    "    'likes': ['max', 'min', lambda x: x.max() - x.min()],\n",
    "}).reset_index()\n",
    "\n",
    "amplitude_stats.columns = ['id', 'label', 'views_max', 'views_min', 'views_amplitude',\n",
    "                           'likes_max', 'likes_min', 'likes_amplitude']\n",
    "\n",
    "# plot amplitude distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# views amplitude\n",
    "ax = axes[0, 0]\n",
    "normal_amp = amplitude_stats[amplitude_stats['label'] == 'normal']['views_amplitude'].dropna()\n",
    "fake_amp = amplitude_stats[amplitude_stats['label'] == 'fake']['views_amplitude'].dropna()\n",
    "ax.hist(normal_amp, bins=50, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "ax.hist(fake_amp, bins=50, alpha=0.6, label='Fake', color='red', density=True)\n",
    "ax.set_xlabel('Views Amplitude (max - min)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Distribution of Views Amplitude', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# likes amplitude\n",
    "ax = axes[0, 1]\n",
    "normal_amp_likes = amplitude_stats[amplitude_stats['label'] == 'normal']['likes_amplitude'].dropna()\n",
    "fake_amp_likes = amplitude_stats[amplitude_stats['label'] == 'fake']['likes_amplitude'].dropna()\n",
    "ax.hist(normal_amp_likes, bins=50, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "ax.hist(fake_amp_likes, bins=50, alpha=0.6, label='Fake', color='red', density=True)\n",
    "ax.set_xlabel('Likes Amplitude (max - min)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Distribution of Likes Amplitude', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# box plot comparison\n",
    "ax = axes[1, 0]\n",
    "box_data = [normal_amp.dropna(), fake_amp.dropna()]\n",
    "ax.boxplot(box_data, labels=['Normal', 'Fake'])\n",
    "ax.set_ylabel('Views Amplitude', fontsize=12)\n",
    "ax.set_title('Views Amplitude Comparison (Box Plot)', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# scatter: max vs min\n",
    "ax = axes[1, 1]\n",
    "normal_data = amplitude_stats[amplitude_stats['label'] == 'normal']\n",
    "fake_data = amplitude_stats[amplitude_stats['label'] == 'fake']\n",
    "ax.scatter(normal_data['views_min'], normal_data['views_max'], \n",
    "           alpha=0.5, label='Normal', color='blue', s=20)\n",
    "ax.scatter(fake_data['views_min'], fake_data['views_max'], \n",
    "           alpha=0.5, label='Fake', color='red', s=20)\n",
    "ax.set_xlabel('Min Views', fontsize=12)\n",
    "ax.set_ylabel('Max Views', fontsize=12)\n",
    "ax.set_title('Min vs Max Views', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_10_amplitude_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Class Balance Analysis\n",
    "\n",
    "Check if the dataset is balanced and suitable for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "label_counts = df['label'].value_counts()\n",
    "label_proportions = df['label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(label_counts)\n",
    "print(\"\\nClass proportions:\")\n",
    "print(label_proportions)\n",
    "\n",
    "# plot class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# count plot\n",
    "axes[0].bar(label_counts.index, label_counts.values, color=['blue', 'red'], alpha=0.7)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xlabel('Label', fontsize=12)\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# add count labels on bars\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v + max(label_counts.values) * 0.01, str(v), \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# proportion pie chart\n",
    "axes[1].pie(label_proportions.values, labels=label_proportions.index, \n",
    "            autopct='%1.1f%%', startangle=90, colors=['blue', 'red'])\n",
    "axes[1].set_title('Class Distribution (Proportion)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_08_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# videos per class\n",
    "videos_per_class = df.groupby('label')['id'].nunique()\n",
    "print(\"\\nNumber of unique videos per class:\")\n",
    "print(videos_per_class)\n",
    "print(f\"\\nRatio (normal/fake): {videos_per_class['normal'] / videos_per_class['fake']:.2f}\")\n",
    "\n",
    "# check if balanced (within reasonable range)\n",
    "balance_ratio = label_proportions['normal'] / label_proportions['fake']\n",
    "print(f\"\\nBalance ratio: {balance_ratio:.2f}\")\n",
    "if 0.5 <= balance_ratio <= 2.0:\n",
    "    print(\"Dataset is reasonably balanced for training.\")\n",
    "else:\n",
    "    print(\"Warning: Dataset may be imbalanced. Consider using class weights or resampling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis\n",
    "\n",
    "Examine correlations between different engagement metrics for normal vs fake patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation matrices\n",
    "normal_df = df[df['label'] == 'normal'][metrics]\n",
    "fake_df = df[df['label'] == 'fake'][metrics]\n",
    "\n",
    "normal_corr = normal_df.corr()\n",
    "fake_corr = fake_df.corr()\n",
    "\n",
    "# plot correlation matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(normal_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, vmin=-1, vmax=1, ax=axes[0], square=True)\n",
    "axes[0].set_title('Normal Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(fake_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, vmin=-1, vmax=1, ax=axes[1], square=True)\n",
    "axes[1].set_title('Fake Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"01_exploration_09_plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# compare correlations\n",
    "print(\"Correlation differences (Fake - Normal):\")\n",
    "corr_diff = fake_corr - normal_corr\n",
    "print(corr_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "Key findings from the exploratory analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Dataset size: {len(df):,} rows, {df['id'].nunique()} unique videos\")\n",
    "print(f\"   - Normal: {label_counts['normal']:,} rows ({label_proportions['normal']:.1%})\")\n",
    "print(f\"   - Fake: {label_counts['fake']:,} rows ({label_proportions['fake']:.1%})\")\n",
    "\n",
    "print(f\"\\n2. Class balance:\")\n",
    "print(f\"   - Ratio (normal/fake): {balance_ratio:.2f}\")\n",
    "if 0.5 <= balance_ratio <= 2.0:\n",
    "    print(\"   - Status: Reasonably balanced\")\n",
    "else:\n",
    "    print(\"   - Status: Imbalanced - consider class weights\")\n",
    "\n",
    "print(f\"\\n3. Pattern distinctness:\")\n",
    "for metric in metrics:\n",
    "    normal_mean = df[df['label'] == 'normal'][metric].mean()\n",
    "    fake_mean = df[df['label'] == 'fake'][metric].mean()\n",
    "    diff_pct = ((fake_mean - normal_mean) / normal_mean) * 100\n",
    "    print(f\"   - {metric.capitalize()}: {diff_pct:+.1f}% difference\")\n",
    "\n",
    "print(f\"\\n4. Data quality:\")\n",
    "print(f\"   - Missing values: {df[metrics].isna().sum().sum()}\")\n",
    "print(f\"   - Negative values: {(df[metrics] < 0).sum().sum()}\")\n",
    "\n",
    "print(f\"\\n5. Conclusion:\")\n",
    "print(\"   - Dataset is ready for feature engineering and modeling\")\n",
    "print(\"   - Clear visual differences between normal and fake patterns\")\n",
    "print(\"   - Temporal patterns show distinct behaviors\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
