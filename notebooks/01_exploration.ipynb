{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis - Engagement Time Series\n",
        "\n",
        "This notebook explores the synthetic engagement time series data to understand:\n",
        "- Distribution of engagement metrics (views, likes, comments, shares)\n",
        "- Differences between normal and fake engagement patterns\n",
        "- Temporal patterns and anomalies\n",
        "- Class balance and data quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# set plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# import project modules\n",
        "from src.data.simulate_timeseries import generate_dataset\n",
        "from src.data.preprocess import load_and_preprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate and Load Data\n",
        "\n",
        "First, we generate a synthetic dataset with normal and fake engagement patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate dataset\n",
        "data_path = project_root / \"data\" / \"raw\" / \"engagement_timeseries.parquet\"\n",
        "\n",
        "# generate if it doesn't exist\n",
        "if not data_path.exists():\n",
        "    print(\"Generating dataset...\")\n",
        "    generate_dataset(\n",
        "        n_normal=100,\n",
        "        n_fake=30,\n",
        "        length_days=30,\n",
        "        frequency=\"H\",\n",
        "        output_path=str(data_path),\n",
        "        output_format=\"parquet\",\n",
        "        random_seed=42\n",
        "    )\n",
        "else:\n",
        "    print(f\"Loading existing dataset from {data_path}\")\n",
        "\n",
        "# load and preprocess\n",
        "df = load_and_preprocess(\n",
        "    file_path=str(data_path),\n",
        "    target_timezone=\"UTC\",\n",
        "    resample_frequency=\"h\",\n",
        "    handle_missing=True,\n",
        "    missing_method=\"forward\",\n",
        "    normalize=False\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nNumber of unique videos: {df['id'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Overview\n",
        "\n",
        "Basic statistics and data quality checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# basic statistics by label\n",
        "print(\"Summary statistics by label:\\n\")\n",
        "print(df.groupby('label')[['views', 'likes', 'comments', 'shares']].describe())\n",
        "\n",
        "# check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df[['views', 'likes', 'comments', 'shares']].isna().sum())\n",
        "\n",
        "# check data types\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Distribution Analysis - Histograms\n",
        "\n",
        "Compare the distribution of engagement metrics between normal and fake patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create histograms for each metric\n",
        "metrics = ['views', 'likes', 'comments', 'shares']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # plot histograms for normal and fake\n",
        "    normal_data = df[df['label'] == 'normal'][metric]\n",
        "    fake_data = df[df['label'] == 'fake'][metric]\n",
        "    \n",
        "    ax.hist(normal_data, bins=50, alpha=0.6, label='Normal', color='blue', density=True)\n",
        "    ax.hist(fake_data, bins=50, alpha=0.6, label='Fake', color='red', density=True)\n",
        "    \n",
        "    ax.set_xlabel(metric.capitalize(), fontsize=12)\n",
        "    ax.set_ylabel('Density', fontsize=12)\n",
        "    ax.set_title(f'Distribution of {metric.capitalize()}', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print statistics\n",
        "print(\"\\nMean values by label:\")\n",
        "print(df.groupby('label')[metrics].mean())\n",
        "print(\"\\nMedian values by label:\")\n",
        "print(df.groupby('label')[metrics].median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Average Temporal Patterns\n",
        "\n",
        "Compare average engagement curves over time for normal vs fake patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalize time for each video (0 to 1)\n",
        "df_normalized_time = df.copy()\n",
        "df_normalized_time['time_normalized'] = df_normalized_time.groupby('id')['timestamp'].transform(\n",
        "    lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0\n",
        ")\n",
        "\n",
        "# create time bins for averaging\n",
        "n_bins = 50\n",
        "df_normalized_time['time_bin'] = pd.cut(df_normalized_time['time_normalized'], bins=n_bins, labels=False)\n",
        "\n",
        "# calculate average curves by label\n",
        "avg_curves = df_normalized_time.groupby(['label', 'time_bin'])[metrics].mean().reset_index()\n",
        "\n",
        "# plot average curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    normal_curve = avg_curves[avg_curves['label'] == 'normal']\n",
        "    fake_curve = avg_curves[avg_curves['label'] == 'fake']\n",
        "    \n",
        "    ax.plot(normal_curve['time_bin'], normal_curve[metric], \n",
        "            label='Normal', linewidth=2, color='blue', marker='o', markersize=4)\n",
        "    ax.plot(fake_curve['time_bin'], fake_curve[metric], \n",
        "            label='Fake', linewidth=2, color='red', marker='s', markersize=4)\n",
        "    \n",
        "    ax.set_xlabel('Normalized Time (0 = start, 1 = end)', fontsize=12)\n",
        "    ax.set_ylabel(f'Average {metric.capitalize()}', fontsize=12)\n",
        "    ax.set_title(f'Average {metric.capitalize()} Over Time', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Overview\n",
        "\n",
        "Basic statistics and data quality checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define metrics\n",
        "metrics = ['views', 'likes', 'comments', 'shares']\n",
        "\n",
        "# basic statistics by label\n",
        "is_fake_col = df.get('is_fake_series', None)\n",
        "if is_fake_col is None:\n",
        "    is_fake_col = (df.get('label', pd.Series(['normal'] * len(df))) == 'fake')\n",
        "df['label'] = is_fake_col.map({True: 'fake', False: 'normal'})\n",
        "\n",
        "print(\"Summary statistics by label:\\n\")\n",
        "print(df.groupby('label')[metrics].describe())\n",
        "\n",
        "# check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df[metrics].isna().sum())\n",
        "\n",
        "# check data types\n",
        "print(\"\\nData types:\")\n",
        "print(df[metrics + ['timestamp']].dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select example videos\n",
        "normal_ids = df[df['label'] == 'normal']['id'].unique()[:3]\n",
        "fake_ids = df[df['label'] == 'fake']['id'].unique()[:3]\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
        "\n",
        "# plot normal examples\n",
        "for idx, video_id in enumerate(normal_ids):\n",
        "    ax = axes[idx, 0]\n",
        "    video_data = df[df['id'] == video_id].sort_values('timestamp')\n",
        "    \n",
        "    ax.plot(video_data['timestamp'], video_data['views'], \n",
        "            label='Views', linewidth=2, color='blue')\n",
        "    ax.plot(video_data['timestamp'], video_data['likes'] * 20, \n",
        "            label='Likes (x20)', linewidth=2, color='green')\n",
        "    ax.plot(video_data['timestamp'], video_data['comments'] * 50, \n",
        "            label='Comments (x50)', linewidth=2, color='orange')\n",
        "    \n",
        "    ax.set_title(f'Normal Video: {video_id}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Timestamp', fontsize=10)\n",
        "    ax.set_ylabel('Engagement', fontsize=10)\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# plot fake examples\n",
        "for idx, video_id in enumerate(fake_ids):\n",
        "    ax = axes[idx, 1]\n",
        "    video_data = df[df['id'] == video_id].sort_values('timestamp')\n",
        "    \n",
        "    ax.plot(video_data['timestamp'], video_data['views'], \n",
        "            label='Views', linewidth=2, color='blue')\n",
        "    ax.plot(video_data['timestamp'], video_data['likes'] * 20, \n",
        "            label='Likes (x20)', linewidth=2, color='green')\n",
        "    ax.plot(video_data['timestamp'], video_data['comments'] * 50, \n",
        "            label='Comments (x50)', linewidth=2, color='orange')\n",
        "    \n",
        "    ax.set_title(f'Fake Video: {video_id}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Timestamp', fontsize=10)\n",
        "    ax.set_ylabel('Engagement', fontsize=10)\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Pattern Distinctness Analysis\n",
        "\n",
        "Analyze how distinct fake patterns are from normal patterns using statistical measures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Engagement Metrics Comparison Heatmap\n",
        "\n",
        "Compare mean engagement metrics between normal and fake patterns using a heatmap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create comparison heatmap\n",
        "comparison_data = df.groupby('label')[metrics].agg(['mean', 'std', 'max', 'min']).T\n",
        "comparison_data.columns = [f\"{col[0]}_{col[1]}\" for col in comparison_data.columns]\n",
        "\n",
        "# create mean values heatmap\n",
        "mean_comparison = df.groupby('label')[metrics].mean().T\n",
        "mean_comparison.columns = ['Normal', 'Fake']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# absolute values\n",
        "sns.heatmap(mean_comparison, annot=True, fmt='.0f', cmap='YlOrRd', \n",
        "            cbar_kws={'label': 'Mean Value'}, ax=axes[0], linewidths=0.5)\n",
        "axes[0].set_title('Mean Engagement Metrics by Label', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Metric', fontsize=12)\n",
        "\n",
        "# normalized (fake/normal ratio)\n",
        "ratio_comparison = (mean_comparison['Fake'] / (mean_comparison['Normal'] + 1e-6)).to_frame('Fake/Normal Ratio')\n",
        "sns.heatmap(ratio_comparison, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
        "            center=1, vmin=0.5, vmax=2, cbar_kws={'label': 'Ratio'}, \n",
        "            ax=axes[1], linewidths=0.5)\n",
        "axes[1].set_title('Fake/Normal Ratio by Metric', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Metric', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean values comparison:\")\n",
        "print(mean_comparison)\n",
        "print(\"\\nFake/Normal ratios:\")\n",
        "print(ratio_comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate per-video statistics\n",
        "video_stats = df.groupby(['id', 'label']).agg({\n",
        "    'views': ['mean', 'std', 'max', 'min'],\n",
        "    'likes': ['mean', 'std', 'max', 'min'],\n",
        "    'comments': ['mean', 'std', 'max', 'min'],\n",
        "    'shares': ['mean', 'std', 'max', 'min']\n",
        "}).reset_index()\n",
        "\n",
        "video_stats.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in video_stats.columns.values]\n",
        "\n",
        "# calculate coefficient of variation (std/mean) as a measure of variability\n",
        "for metric in metrics:\n",
        "    mean_col = f'{metric}_mean'\n",
        "    std_col = f'{metric}_std'\n",
        "    cv_col = f'{metric}_cv'\n",
        "    video_stats[cv_col] = video_stats[std_col] / (video_stats[mean_col] + 1e-6)\n",
        "\n",
        "# compare statistics between normal and fake\n",
        "print(\"Per-video statistics comparison:\\n\")\n",
        "comparison_cols = [col for col in video_stats.columns if any(m in col for m in metrics) and ('mean' in col or 'cv' in col)]\n",
        "print(video_stats.groupby('label')[comparison_cols].mean())\n",
        "\n",
        "# plot comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    normal_cv = video_stats[video_stats['label'] == 'normal'][f'{metric}_cv']\n",
        "    fake_cv = video_stats[video_stats['label'] == 'fake'][f'{metric}_cv']\n",
        "    \n",
        "    ax.boxplot([normal_cv.dropna(), fake_cv.dropna()], \n",
        "               labels=['Normal', 'Fake'])\n",
        "    ax.set_ylabel('Coefficient of Variation', fontsize=12)\n",
        "    ax.set_title(f'{metric.capitalize()} Variability (CV)', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Correlation Heatmaps - Normal vs Fake\n",
        "\n",
        "Compare correlation patterns between engagement metrics for normal and fake patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute correlation matrices\n",
        "normal_df = df[df['label'] == 'normal'][metrics]\n",
        "fake_df = df[df['label'] == 'fake'][metrics]\n",
        "\n",
        "normal_corr = normal_df.corr()\n",
        "fake_corr = fake_df.corr()\n",
        "\n",
        "# plot correlation heatmaps side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# normal correlation heatmap\n",
        "sns.heatmap(normal_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, vmin=-1, vmax=1, square=True, ax=axes[0],\n",
        "            cbar_kws={'label': 'Correlation'})\n",
        "axes[0].set_title('Normal Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
        "axes[0].set_yticklabels(axes[0].get_yticklabels(), rotation=0)\n",
        "\n",
        "# fake correlation heatmap\n",
        "sns.heatmap(fake_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, vmin=-1, vmax=1, square=True, ax=axes[1],\n",
        "            cbar_kws={'label': 'Correlation'})\n",
        "axes[1].set_title('Fake Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
        "axes[1].set_yticklabels(axes[1].get_yticklabels(), rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# difference heatmap\n",
        "corr_diff = fake_corr - normal_corr\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "sns.heatmap(corr_diff, annot=True, fmt='.2f', cmap='RdBu_r', \n",
        "            center=0, vmin=-0.5, vmax=0.5, square=True, ax=ax,\n",
        "            cbar_kws={'label': 'Difference (Fake - Normal)'})\n",
        "ax.set_title('Correlation Difference: Fake - Normal', fontsize=14, fontweight='bold')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation differences (Fake - Normal):\")\n",
        "print(corr_diff.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Class Balance Analysis\n",
        "\n",
        "Check if the dataset is balanced and suitable for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class distribution\n",
        "label_counts = df['label'].value_counts()\n",
        "label_proportions = df['label'].value_counts(normalize=True)\n",
        "\n",
        "print(\"Class distribution:\")\n",
        "print(label_counts)\n",
        "print(\"\\nClass proportions:\")\n",
        "print(label_proportions)\n",
        "\n",
        "# plot class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# count plot\n",
        "axes[0].bar(label_counts.index, label_counts.values, color=['blue', 'red'], alpha=0.7)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_xlabel('Label', fontsize=12)\n",
        "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# add count labels on bars\n",
        "for i, v in enumerate(label_counts.values):\n",
        "    axes[0].text(i, v + max(label_counts.values) * 0.01, str(v), \n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# proportion pie chart\n",
        "axes[1].pie(label_proportions.values, labels=label_proportions.index, \n",
        "            autopct='%1.1f%%', startangle=90, colors=['blue', 'red'])\n",
        "axes[1].set_title('Class Distribution (Proportion)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# videos per class\n",
        "videos_per_class = df.groupby('label')['id'].nunique()\n",
        "print(\"\\nNumber of unique videos per class:\")\n",
        "print(videos_per_class)\n",
        "print(f\"\\nRatio (normal/fake): {videos_per_class['normal'] / videos_per_class['fake']:.2f}\")\n",
        "\n",
        "# check if balanced (within reasonable range)\n",
        "balance_ratio = label_proportions['normal'] / label_proportions['fake']\n",
        "print(f\"\\nBalance ratio: {balance_ratio:.2f}\")\n",
        "if 0.5 <= balance_ratio <= 2.0:\n",
        "    print(\"Dataset is reasonably balanced for training.\")\n",
        "else:\n",
        "    print(\"Warning: Dataset may be imbalanced. Consider using class weights or resampling.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Correlation Analysis\n",
        "\n",
        "Examine correlations between different engagement metrics for normal vs fake patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate correlation matrices\n",
        "normal_df = df[df['label'] == 'normal'][metrics]\n",
        "fake_df = df[df['label'] == 'fake'][metrics]\n",
        "\n",
        "normal_corr = normal_df.corr()\n",
        "fake_corr = fake_df.corr()\n",
        "\n",
        "# plot correlation matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.heatmap(normal_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, vmin=-1, vmax=1, ax=axes[0], square=True)\n",
        "axes[0].set_title('Normal Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "sns.heatmap(fake_corr, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, vmin=-1, vmax=1, ax=axes[1], square=True)\n",
        "axes[1].set_title('Fake Engagement - Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# compare correlations\n",
        "print(\"Correlation differences (Fake - Normal):\")\n",
        "corr_diff = fake_corr - normal_corr\n",
        "print(corr_diff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Conclusions\n",
        "\n",
        "Key findings from the exploratory analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"EDA SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1. Dataset size: {len(df):,} rows, {df['id'].nunique()} unique videos\")\n",
        "print(f\"   - Normal: {label_counts['normal']:,} rows ({label_proportions['normal']:.1%})\")\n",
        "print(f\"   - Fake: {label_counts['fake']:,} rows ({label_proportions['fake']:.1%})\")\n",
        "\n",
        "print(f\"\\n2. Class balance:\")\n",
        "print(f\"   - Ratio (normal/fake): {balance_ratio:.2f}\")\n",
        "if 0.5 <= balance_ratio <= 2.0:\n",
        "    print(\"   - Status: Reasonably balanced\")\n",
        "else:\n",
        "    print(\"   - Status: Imbalanced - consider class weights\")\n",
        "\n",
        "print(f\"\\n3. Pattern distinctness:\")\n",
        "for metric in metrics:\n",
        "    normal_mean = df[df['label'] == 'normal'][metric].mean()\n",
        "    fake_mean = df[df['label'] == 'fake'][metric].mean()\n",
        "    diff_pct = ((fake_mean - normal_mean) / normal_mean) * 100\n",
        "    print(f\"   - {metric.capitalize()}: {diff_pct:+.1f}% difference\")\n",
        "\n",
        "print(f\"\\n4. Data quality:\")\n",
        "print(f\"   - Missing values: {df[metrics].isna().sum().sum()}\")\n",
        "print(f\"   - Negative values: {(df[metrics] < 0).sum().sum()}\")\n",
        "\n",
        "print(f\"\\n5. Conclusion:\")\n",
        "print(\"   - Dataset is ready for feature engineering and modeling\")\n",
        "print(\"   - Clear visual differences between normal and fake patterns\")\n",
        "print(\"   - Temporal patterns show distinct behaviors\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
